import torch
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import glob
import time
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from omegaconf import OmegaConf
from ldm.util import instantiate_from_config
from cldm.model import load_state_dict
from ldm.modules.attention import SpatialTransformer
from cldm.ddim_hacked import DDIMSampler

# ==================================================================================
# [1] ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ==================================================================================
class LightingDataset(Dataset):
    def __init__(self, root_dir):
        self.root_dir = root_dir
        self.data_pairs = []
        
        print(f"ğŸ” '{root_dir}' ë°ì´í„° ìŠ¤ìº” ì¤‘...")
        # ëª¨ë“  í•˜ìœ„ í´ë” íƒìƒ‰
        for root, dirs, files in os.walk(root_dir):
            off_f, on_f = None, None
            for f in sorted(files):
                lower = f.lower()
                if not lower.endswith(('.jpg', '.jpeg', '.png', '.avif', '.webp')): continue #ì´ë¯¸ì§€ë§Œ íƒìƒ‰
                
                if 'off' in lower: off_f = os.path.join(root, f)
                elif 'on' in lower and 'color' not in lower: on_f = os.path.join(root, f)
            
            if off_f and on_f: #on, offê°€ ë‘˜ ë‹¤ ìˆëŠ” ë””ë ‰í† ë¦¬ ì°¾ê¸°
                self.data_pairs.append({'off': off_f, 'on': on_f})

        print(f"  âœ… ì´ {len(self.data_pairs)}ê°œì˜ ìŒì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    def __len__(self): return len(self.data_pairs)

    def __getitem__(self, idx):
        item = self.data_pairs[idx]
        
        src_off = Image.open(item['off']).convert("RGB").resize((512, 512), Image.BICUBIC) #offì´ë¯¸ì§€ ë¡œë“œ
        src_on  = Image.open(item['on']).convert("RGB").resize((512, 512), Image.BICUBIC) #onì´ë¯¸ì§€ ë¡œë“œ
        
        # White Reference ìƒì„±
        white_ref = Image.new("RGB", (512, 512), (255, 255, 255))
        
        t_off   = torch.from_numpy(np.array(src_off).astype(np.float32)/255.0).permute(2,0,1) #ì •ê·œí™”
        t_white = torch.from_numpy(np.array(white_ref).astype(np.float32)/255.0).permute(2,0,1)
        t_on    = torch.from_numpy(np.array(src_on).astype(np.float32)/255.0).permute(2,0,1)
        
        # Hint: OFF + White = control Netìœ¼ë¡œ ì „ë‹¬
        hint = torch.cat((t_off, t_white), dim=0)
        # Target: ON = ì •ë‹µê°’
        jpg = (t_on * 2.0) - 1.0
        
        return {"jpg": jpg, "hint": hint}

# ==================================================================================
# [2] ìœ í‹¸ë¦¬í‹°: ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
# ==================================================================================
def plot_loss_graph(train_losses, val_losses, save_path="loss_graph.png"):
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Train Loss', color='blue', alpha=0.7)
    plt.plot(val_losses, label='Validation Loss', color='orange', linewidth=2)
    plt.title('Training & Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(save_path)
    plt.close()
    print(f"ğŸ“Š ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")

# ==================================================================================
# [3] í•™ìŠµ ë©”ì¸ í•¨ìˆ˜
# ==================================================================================
def train_step_safe():
    # --- ì„¤ì • ---
    # ê²½ë¡œ
    train_root = "./images/train" # train ë°ì´í„° ë””ë ‰í† ë¦¬
    val_root   = "./images/validation" # validation ë°ì´í„° ë””ë ‰í† ë¦¬
    
    # ì €ì¥ì†Œ
    save_dir = "./crossattn_checkpoints" # ckpt ì €ì¥ ìœ„ì¹˜
    sample_dir = "./train_log_images" # ê²€ì¦ ì´ë¯¸ì§€ ì €ì¥ ìœ„ì¹˜
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(sample_dir, exist_ok=True)
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    batch_size = 5  
    epochs = 50
    learning_rate = 1e-5
    
    print("\n Cross-Attention í•™ìŠµ")
    print(f"   - Batch Size: {batch_size}")
    print(f"   - Max Epochs: {epochs}")

    # 1. ëª¨ë¸ ë¡œë“œ
    config = OmegaConf.load('./models/cldm_v21_LumiNet.yaml')
    config.model.params.control_stage_config.params.use_checkpoint = False
    config.model.params.unet_config.params.use_checkpoint = False
    
    model = instantiate_from_config(config.model).cpu()
    model.add_new_layers()
    
    # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    if os.path.exists("./ckpt/LumiNet.ckpt"):
        model.load_state_dict(load_state_dict("./ckpt/LumiNet.ckpt", 'cpu'), strict=False)
        print("ğŸ“¦ ê¸°ë³¸ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œë¨")
    
    model.train().cuda()

    # 2. í•™ìŠµ ëŒ€ìƒ ì„¤ì • (Cross-Attention Only)
    trainable_params = []
    for param in model.parameters(): param.requires_grad = False # ëª¨ë“  íŒŒë¼ë¯¸í„° ë™ê²°
    
    attn_count = 0
    for name, module in model.named_modules():
        if isinstance(module, SpatialTransformer): # Diffusionì—ì„œ cross attentionë§Œ í•™ìŠµ ëŒ€ìƒì— í¬í•¨
            for param in module.parameters():
                param.requires_grad = True # cross attentionë§Œ ë™ê²° í•´ì œ
                trainable_params.append(param)
            attn_count += 1
            
    print(f"ğŸ”“ Cross-Attention Layer ({attn_count}ê°œ) í•´ë™ ì™„ë£Œ")
    
    optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate) # optimizer ì„¤ì •
    
    # 3. ë°ì´í„° ë¡œë”
    train_dataset = LightingDataset(train_root)
    val_dataset = LightingDataset(val_root)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    
    # ê²€ì¦ìš© ê³ ì • ìƒ˜í”Œ (í•™ìŠµ ì¤‘ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ validationì—ì„œ 1ê°œ í™•ë³´)
    fixed_val_batch = next(iter(DataLoader(val_dataset, batch_size=1, shuffle=False)))

    # -------------------------------------------------------
    # [Resume Logic] ì¤‘ë‹¨ëœ í•™ìŠµ ì´ì–´í•˜ê¸°
    # -------------------------------------------------------
    start_epoch = 0
    best_val_loss = float('inf')
    train_loss_history = []
    val_loss_history = []
    
    # ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
    resume_ckpt = os.path.join(save_dir, "last_checkpoint.pth")
    if os.path.exists(resume_ckpt):
        print(f" ë³µêµ¬ íŒŒì¼ ë°œê²¬! í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤: {resume_ckpt}")
        checkpoint = torch.load(resume_ckpt)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_val_loss = checkpoint['best_val_loss']
        train_loss_history = checkpoint['train_loss_history']
        val_loss_history = checkpoint['val_loss_history']
        print(f"   â–¶ {start_epoch} Epochë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("ğŸ†• ìƒˆë¡œìš´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # -------------------------------------------------------
    # í•™ìŠµ ë£¨í”„
    # -------------------------------------------------------
    for epoch in range(start_epoch, epochs):
        epoch_start_time = time.time()
        model.train()
        train_loss_sum = 0
        
        # (A) Training
        for batch in train_loader:
            x = batch["jpg"].cuda() # ì •ë‹µê°’(on)
            hint = batch["hint"].cuda() # off + white
            
            with torch.no_grad():
                z = model.get_first_stage_encoding(model.encode_first_stage(x)).detach() 
                # ì •ë‹µ ì´ë¯¸ì§€(x)ë¥¼ VAEë¥¼ í†µí•´ Latent ê³µê°„(z)ìœ¼ë¡œ ì••ì¶•
            
            c = {"c_concat": [hint], "c_crossattn": [model.get_learned_conditioning([""] * x.shape[0])]} 
            # c_concat(ì´ë¯¸ì§€ íŒíŠ¸)ì€ ControlNetìœ¼ë¡œ, c_crossattn(ë”ë¯¸ í”„ë¡¬í”„íŠ¸)ì€ Diffusion ëª¨ë¸ë¡œ ì „ë‹¬
            t = torch.randint(0, model.num_timesteps, (z.shape[0],), device=model.device).long()
            
            loss, _ = model.p_losses(z, c, t) #./ldm/models/diffusion/ddpm.pyì˜ p_lossesí•¨ìˆ˜ í˜¸ì¶œ
            # Latent(z)ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ê³ , ëª¨ë¸ì´ ê·¸ ë…¸ì´ì¦ˆë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ ê³„ì‚° (MSE Loss)
            # ë…¸ì´ì¦ˆ ì˜ˆì¸¡ì„ ì˜ í• ìˆ˜ë¡ ì‹¤ì œê°’ê³¼ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ ìƒì„±
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss_sum += loss.item()
            
        avg_train_loss = train_loss_sum / len(train_loader)
        train_loss_history.append(avg_train_loss)
        
        # (B) Validation
        model.eval()
        val_loss_sum = 0
        with torch.no_grad():
            for batch in val_loader:
                x = batch["jpg"].cuda() # ì •ë‹µê°’
                hint = batch["hint"].cuda() # off + white
                z = model.get_first_stage_encoding(model.encode_first_stage(x)).detach() # ì •ë‹µê°’
                c = {"c_concat": [hint], "c_crossattn": [model.get_learned_conditioning([""] * x.shape[0])]}
                t = torch.randint(0, model.num_timesteps, (z.shape[0],), device=model.device).long()
                
                # Validationì€ Lossë§Œ ê³„ì‚°
                loss, _ = model.p_losses(z, c, t)
                val_loss_sum += loss.item()
                
        avg_val_loss = val_loss_sum / len(val_loader)
        val_loss_history.append(avg_val_loss)
        
        # (C) ê²°ê³¼ ì¶œë ¥
        elapsed = time.time() - epoch_start_time
        print(f"Epoch {epoch+1}/{epochs} ({elapsed:.1f}s) | Train: {avg_train_loss:.5f} | Val: {avg_val_loss:.5f}")

        # (D) ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì•ˆì „ì¥ì¹˜)
        # 1. "Last" Checkpoint (ë§¤ë²ˆ ë®ì–´ì“°ê¸° - ë³µêµ¬ìš©)
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_val_loss': best_val_loss,
            'train_loss_history': train_loss_history,
            'val_loss_history': val_loss_history
        }, resume_ckpt)
        
        # 2. "Periodic" Checkpoint (10 Epoch ë§ˆë‹¤)
        if (epoch + 1) % 10 == 0:
            periodic_path = os.path.join(save_dir, f"epoch_{epoch+1:03d}.ckpt")
            torch.save(model.state_dict(), periodic_path)
            print(f"   ğŸ’¾ ì •ê¸° ì €ì¥ ì™„ë£Œ: {periodic_path}")
            
            # ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
            plot_loss_graph(train_loss_history, val_loss_history, "loss_graph_crossattn.png")

        # 3. "Best" Checkpoint (ì‹ ê¸°ë¡ ë‹¬ì„± ì‹œ)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            print(f"   ğŸŒŸ Best Validation Loss! -> best_crossattn.ckpt ì €ì¥")
            torch.save(model.state_dict(), os.path.join(save_dir, "best_crossattn.ckpt"))

        # (E) ê²€ì¦ ì´ë¯¸ì§€ ìƒì„± (ë§¤ Epoch)
        # í•™ìŠµì´ ì˜ ë˜ê³  ìˆëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸ (í˜„ì¬ ê°€ì¤‘ì¹˜ë¡œ ì¶”ë¡ )
        sampler = DDIMSampler(model)
        with torch.no_grad():
            c_cat = fixed_val_batch["hint"].cuda()
            c = model.get_unconditional_conditioning(1)
            cond = {"c_concat": [c_cat], "c_crossattn": [c]}
            shape = (4, 512 // 8, 512 // 8)
            
            samples, _ = sampler.sample(50, 1, shape, cond, verbose=False, unconditional_guidance_scale=9.0)
            x_sample = model.decode_first_stage(samples)
            x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0)
            x_sample = x_sample.cpu().permute(0, 2, 3, 1).numpy()[0] * 255
            
            img_path = os.path.join(sample_dir, f"val_ep{epoch+1:03d}.jpg")
            cv2.imwrite(img_path, cv2.cvtColor(x_sample.astype(np.uint8), cv2.COLOR_RGB2BGR))

    # ìµœì¢… ê·¸ë˜í”„
    plot_loss_graph(train_loss_history, val_loss_history, "loss_graph_crossattn_final.png")
    print("\n Cross Attention í•™ìŠµì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == '__main__':
    train_step_safe()