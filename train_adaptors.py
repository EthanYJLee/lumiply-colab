'''
train_adaptors.pyëŠ” ControlNetì˜ latent intrinsic encoder(light_encoder)ì™€ MLP Adaptor(adaptor)ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
ë³¸ íŠœë‹ì˜ ëª©ì ì€ ì¡°ëª…ì •ë³´ì— ì›í•˜ëŠ” ìƒ‰ìƒì„ ì „ì´ì‹œí‚¤ë„ë¡í•˜ëŠ”ê²ƒì…ë‹ˆë‹¤.
ë°ì´í„°ì…‹ì€ ê°™ì€ ê³µê°„ì´ì§€ë§Œ ë‹¤ë¥¸ ì¡°ëª…ì¡°ê±´ì„ ê°€ì§„ 'off'ê³¼ 'color' ì´ë¯¸ì§€ë¥¼ ê°€ì§„ í´ë”ë“¤ì„ ë³´ìœ í•œ ìƒíƒœì—ì„œ í•™ìŠµì„ ì§„í–‰í•˜ì—¬ì•¼í•©ë‹ˆë‹¤. 
í•™ìŠµ ì¤‘ì—ëŠ” 'off'ì™€ 'white_ref'ë¥¼ ì…ë ¥ë°›ì•„ U-netì—ì„œ ìƒì„±ëœ ë…¸ì´ì¦ˆì™€ ì‹¤ì œê°’ì¸ 'color'ì—ì„œ ìƒì„±ëœ ë…¸ì´ì¦ˆë¥¼ ë¹„êµí•´ Lossë¥¼ ì •ì˜í•˜ê³ 
Lossê°€ ì¤„ì–´ë“œëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰í•˜ê²Œë©ë‹ˆë‹¤.
ì—¬ê¸°ì„œ 'off'ëŠ” ì¡°ëª…ì´ êº¼ì§„ì‚¬ì§„, 'color'ëŠ” ìƒ‰ì´ ë‹¤ë¥¸ ì¡°ëª…ì´ ì¼œì§„ì‚¬ì§„, 'white_ref'ëŠ” í°ìƒ‰ì´ë¯¸ì§€ë¡œ 'ì¡°ëª…ì„ ì¼œ'ë¼ëŠ” ì¼ì¢…ì˜ ì…ë ¥ì‹ í˜¸ì…ë‹ˆë‹¤.
ë…¸ì´ì¦ˆê°€ ì„œë¡œ ê°™ìœ¼ë©´ ê°™ì€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸°ë•Œë¬¸ì— Lossê°€ ì ìœ¼ë©´ onê³¼ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ ë‚´ê³  ìˆë‹¤ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë§¤ epochë§ˆë‹¤ í˜„ì¬ ê°€ì¤‘ì¹˜ë¥¼ pthíŒŒì¼ë¡œ ì €ì¥í•˜ë©°, í˜„ì¬ ê°€ì¤‘ì¹˜ë¡œ ì¶”ë¡ í•œ ê²€ì¦ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
'''




import torch
import os
import cv2
import numpy as np
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from omegaconf import OmegaConf
from ldm.util import instantiate_from_config
from cldm.model import load_state_dict
from cldm.ddim_hacked import DDIMSampler

# ==================================================================================
# [1] ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# offì™€ color í˜ì–´ ìˆëŠ” í´ë” ì°¾ê¸°
# ==================================================================================
class ColorLightDataset(Dataset):
    def __init__(self, root_dir):
        self.root_dir = root_dir
        self.data_pairs = []
        
        print(f"\nğŸ“‚ '{root_dir}' ë°ì´í„° ìŠ¤ìº” ì¤‘...")
        
        # dataset êµ¬ì¡°: root/00001/off.jpg, color.jpg
        for root, dirs, files in os.walk(root_dir): # offì™€ colorê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ìŠ¤ìº”
            if 'off.jpg' in files and 'red.jpg' in files: # color ë³€ê²½ ì‹œ ì—¬ê¸° ìˆ˜ì • ex) red > yellow / íŒŒì¼ëª… colorì•„ë‹˜ ì£¼ì˜
                off_path = os.path.join(root, 'off.jpg')
                color_path = os.path.join(root, 'red.jpg') # color ë³€ê²½ ì‹œ ì—¬ê¸° ìˆ˜ì • ex) red > yellow / íŒŒì¼ëª… colorì•„ë‹˜ ì£¼ì˜
                self.data_pairs.append({'off': off_path, 'color': color_path})

        if len(self.data_pairs) > 0:
            print(f"  âœ… ì´ {len(self.data_pairs)}ê°œì˜ ë°ì´í„° ìŒì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        else:
            print("  âŒ ê²½ê³ : ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    def __len__(self):
        return len(self.data_pairs)

    def __getitem__(self, idx):
        item = self.data_pairs[idx]
        
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ë¦¬ì‚¬ì´ì¦ˆ (512x512)
        src_off = Image.open(item['off']).convert("RGB").resize((512, 512), Image.BICUBIC) # off ì´ë¯¸ì§€ ë¡œë“œ
        tgt_color = Image.open(item['color']).convert("RGB").resize((512, 512), Image.BICUBIC) # color ì´ë¯¸ì§€ ë¡œë“œ
        white_ref = Image.new("RGB", (512, 512), (255, 255, 255)) #white_ref ìƒì„±

        # 1. Hint êµ¬ì„± (off, white_reference)
        t_off = torch.from_numpy(np.array(src_off).astype(np.float32)/255.0).permute(2,0,1)
        t_white = torch.from_numpy(np.array(white_ref).astype(np.float32)/255.0).permute(2,0,1)
        
        # Hint: OFF + White = control Netìœ¼ë¡œ ì „ë‹¬
        hint = torch.cat((t_off, t_white), dim=0)
        # color ì´ë¯¸ì§€ (ì •ë‹µê°’)
        t_color = torch.from_numpy(np.array(tgt_color).astype(np.float32)/255.0).permute(2,0,1)
        jpg = (t_color * 2.0) - 1.0
        
        return {"jpg": jpg, "hint": hint}

# ==================================================================================
# [2] í•™ìŠµ ë©”ì¸ í•¨ìˆ˜
# ==================================================================================
def train_color():
    # --- ì„¤ì • ---
    save_dir = "./adaptors_red"  # color ë³€ê²½ ì‹œ ì—¬ê¸° ìˆ˜ì •     
    log_img_dir = "./train_log_red"   # color ë³€ê²½ ì‹œ ì—¬ê¸° ìˆ˜ì •
    dataset_root = "./images"    # í•™ìŠµë°ì´í„° ë””ë ‰í† ë¦¬
    
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(log_img_dir, exist_ok=True)
    
    print("\n [Color Light Training] í•™ìŠµ ì¤€ë¹„...")
    
    # 1. ëª¨ë¸ ë¡œë“œ
    config = OmegaConf.load('./models/cldm_v21_LumiNet.yaml')
    config.model.params.control_stage_config.params.use_checkpoint = False
    config.model.params.unet_config.params.use_checkpoint = False
    
    model = instantiate_from_config(config.model).cpu()
    model.add_new_layers() # Layer ì´ˆê¸°í™”
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    ckpt_path = "./ckpt/trained_crossattn.ckpt" # í•™ìŠµëœ cross_attention ckpt
    if os.path.exists(ckpt_path):
        model.load_state_dict(load_state_dict(ckpt_path, 'cpu'), strict=False)
        print("ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    model.train().cuda()
    
    # 2. í•™ìŠµ ëŒ€ìƒ ì„¤ì • (Adaptor + Encoder)
    trainable_params = []
    for param in model.parameters(): param.requires_grad = False # ì „ì²´ íŒŒë¼ë¯¸í„° ë™ê²°
    
    if hasattr(model.control_model, 'prior_extracter'):
        adaptor = model.control_model.prior_extracter.light_decoder
        encoder = model.control_model.prior_extracter.model_latents.light_encoder
        # light_encoderì™€ adaptorëŠ” ë™ê²°í•´ì œ
        for param in adaptor.parameters(): param.requires_grad = True; trainable_params.append(param) #adaptor ë™ê²°í•´ì²´
        for param in encoder.parameters(): param.requires_grad = True; trainable_params.append(param) #encoder ë™ê²°í•´ì²´
            
        print(" Light Encoder & Adaptor í•™ìŠµ ëª¨ë“œ ì„¤ì •ë¨")

    optimizer = torch.optim.AdamW(trainable_params, lr=1e-5) # optimizer ì„¤ì •
    

    
    # 3. ë°ì´í„° ë¡œë”
    train_dataset = ColorLightDataset(os.path.join(dataset_root, 'train')) #./images/train
    val_dataset = ColorLightDataset(os.path.join(dataset_root, 'validation')) # ./images/validation
    
    train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=2) # batch sizeì„¤ì •
    val_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=2) # batch sizeì„¤ì •
    
    # ê²€ì¦ìš© ê³ ì • ìƒ˜í”Œ (í•™ìŠµ ì¤‘ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ validationì—ì„œ 1ê°œ í™•ë³´)
    viz_batch = next(iter(DataLoader(val_dataset, batch_size=1, shuffle=False))) if len(val_dataset) > 0 else None

    if len(train_dataset) == 0: print("âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return

    epochs = 50 # epoch ì„¤ì •

    print(f"\n í•™ìŠµ ë£¨í”„ ì‹œì‘ (Total Epochs: {epochs})")
    
    for epoch in range(epochs):
        # ----------------------
        # [A] Train Loop
        # ----------------------
        model.train()
        train_loss = 0
        for batch in train_dataloader:
            x = batch["jpg"].cuda()     # ì •ë‹µê°’(color)
            hint = batch["hint"].cuda() # off + white
            
            with torch.no_grad():
                z = model.get_first_stage_encoding(model.encode_first_stage(x)).detach()
                # ì •ë‹µ ì´ë¯¸ì§€(x)ë¥¼ VAEë¥¼ í†µí•´ Latent ê³µê°„(z)ìœ¼ë¡œ ì••ì¶•
            
            c = {"c_concat": [hint], "c_crossattn": [model.get_learned_conditioning([""] * x.shape[0])]}
            # c_concat(ì´ë¯¸ì§€ íŒíŠ¸)ì€ ControlNetìœ¼ë¡œ, c_crossattn(ë”ë¯¸ í”„ë¡¬í”„íŠ¸)ì€ Diffusion ëª¨ë¸ë¡œ ì „ë‹¬

            loss, _ = model(z, c) #./ldm/models/diffusion/ddpm.pyì˜ p_lossesí•¨ìˆ˜ í˜¸ì¶œ
            # Latent(z)ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ê³ , ëª¨ë¸ì´ ê·¸ ë…¸ì´ì¦ˆë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ ê³„ì‚° (MSE Loss)
            # ë…¸ì´ì¦ˆ ì˜ˆì¸¡ì„ ì˜ í• ìˆ˜ë¡ ì‹¤ì œê°’ê³¼ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ ìƒì„±
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
        avg_train_loss = train_loss / len(train_dataloader)

        # ----------------------
        # [B] Validation Loop (Loss ê³„ì‚°)
        # ----------------------
        model.eval()
        val_loss = 0
        if len(val_dataloader) > 0:
            with torch.no_grad():
                for batch in val_dataloader:
                    x_val = batch["jpg"].cuda() #ì •ë‹µê°’
                    hint_val = batch["hint"].cuda() #off + white
                    
                    z_val = model.get_first_stage_encoding(model.encode_first_stage(x_val)).detach()
                    c_val = {"c_concat": [hint_val], "c_crossattn": [model.get_learned_conditioning([""] * x_val.shape[0])]}
                    
                    # Validation Loss ê³„ì‚°
                    v_loss, _ = model(z_val, c_val)
                    val_loss += v_loss.item()
            
            avg_val_loss = val_loss / len(val_dataloader)
        else:
            avg_val_loss = 0.0

        print(f"Epoch {epoch+1:03d}/{epochs} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}")

        # ----------------------
        # [C] ë§¤ ì—í¬í¬ í˜„ì¬ ê°€ì¤‘ì¹˜ pthë¡œ ì €ì¥
        # ----------------------
        save_name = f"color_epoch_{epoch+1:03d}.pth"
        save_path = os.path.join(save_dir, save_name)
        
        save_dict = {
            'light_encoder': encoder.state_dict(),
            'light_decoder': adaptor.state_dict()
        }
        torch.save(save_dict, save_path)
        print(f"  {save_name} ì €ì¥ ì™„ë£Œ")

        # ----------------------
        # [D] ë§¤ ì—í¬í¬ ê²€ì¦ ì´ë¯¸ì§€ ìƒì„±
        # í•™ìŠµì´ ì˜ ë˜ê³  ìˆëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸ (í˜„ì¬ ê°€ì¤‘ì¹˜ë¡œ ì¶”ë¡ )
        # ----------------------
        if viz_batch is not None:
            sampler = DDIMSampler(model)
            with torch.no_grad():
                c_cat = viz_batch["hint"].cuda() # off + white
                # Unconditional Conditioning
                c_uncond = model.get_unconditional_conditioning(c_cat.shape[0])
                cond = {"c_concat": [c_cat], "c_crossattn": [c_uncond]} # controlNetìœ¼ë¡œ ì´ë™
                
                # Sampling
                shape = (4, 512 // 8, 512 // 8)
                samples, _ = sampler.sample(50, 1, shape, cond, verbose=False, unconditional_guidance_scale=9.0)
                #Diffusionì—ì„œ ì¶”ë¡ 
                
                # Decoding
                x_sample = model.decode_first_stage(samples)
                x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0)
                x_sample = x_sample.cpu().permute(0, 2, 3, 1).numpy()[0] * 255
                
                # Image Saving
                img_save_path = os.path.join(log_img_dir, f"val_epoch_{epoch+1:03d}.jpg")
                cv2.imwrite(img_save_path, cv2.cvtColor(x_sample.astype(np.uint8), cv2.COLOR_RGB2BGR))
                print(f"   ğŸ“¸ ê²€ì¦ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {img_save_path}")

    print("\nğŸ‰ color í•™ìŠµ ì™„ë£Œ!")

if __name__ == '__main__':
    train_color()