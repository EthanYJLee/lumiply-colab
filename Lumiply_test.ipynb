{"cells":[{"cell_type":"code","execution_count":106,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1765206295730,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"},"user_tz":-540},"id":"ed115ed6"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":107,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1765206295739,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"},"user_tz":-540},"id":"yWtRWFoiLvVV","outputId":"8870306f-db8a-4688-b7b2-124cbcc665f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/LumiNet_Files/LumiNet-lee\n"]}],"source":["%cd /content/drive/MyDrive/LumiNet_Files/LumiNet-lee/"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4967,"status":"ok","timestamp":1765206300707,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"},"user_tz":-540},"id":"OMYjtM_bLyVT","outputId":"d2501993-403b-4a85-db05-089cc05a2f6a","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: open-clip-torch==2.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.0.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.8.0)\n","Requirement already satisfied: positional-encodings in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (6.0.4)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (4.12.0.88)\n","Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.8.1)\n","Requirement already satisfied: OmegaConf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.3.0)\n","Requirement already satisfied: torchviz in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (0.0.3)\n","Requirement already satisfied: pytorch-lightning==1.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.5.0)\n","Requirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.57.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\n","Requirement already satisfied: xformers==0.0.32.post2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.0.32.post2)\n","Requirement already satisfied: share in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (1.0.4)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==2.0.1->-r requirements.txt (line 1)) (0.23.0)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==2.0.1->-r requirements.txt (line 1)) (6.3.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==2.0.1->-r requirements.txt (line 1)) (2025.11.3)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==2.0.1->-r requirements.txt (line 1)) (0.36.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2.0.2)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.0.0)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (6.0.3)\n","Requirement already satisfied: fsspec!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2025.3.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2.19.0)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.8.2)\n","Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (0.3.1)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (25.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (4.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.8.93)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.8.90)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.8.90)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.8.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (11.3.3.83)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (10.3.9.90)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (11.7.3.90)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.5.8.93)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.8.90)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.8.93)\n","Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (1.13.1.3)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.4.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from OmegaConf->-r requirements.txt (line 6)) (4.9.3)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchviz->-r requirements.txt (line 7)) (0.21)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 9)) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 9)) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 9)) (0.7.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (3.13.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch==2.0.1->-r requirements.txt (line 1)) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.76.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (3.10)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (5.29.5)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (3.1.3)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>=0.4.1->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (0.15.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open-clip-torch==2.0.1->-r requirements.txt (line 1)) (0.2.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (2025.11.12)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open-clip-torch==2.0.1->-r requirements.txt (line 1)) (11.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.22.0)\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":109,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4RfhQXbL5Cl","outputId":"4b4aa7f9-7078-49f0-9c34-fe7874d72d18","collapsed":true,"executionInfo":{"status":"ok","timestamp":1765206332797,"user_tz":-540,"elapsed":32085,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì‹œì‘... (ìµœì´ˆ 1íšŒ)\n","ControlLDM: Running in v-prediction mode\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","DiffusionWrapper has 1042.99 M params.\n","making attention of type 'vanilla-xformers' with 512 in_channels\n","building MemoryEfficientAttnBlock with 512 in_channels...\n","Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n","making attention of type 'vanilla-xformers' with 512 in_channels\n","building MemoryEfficientAttnBlock with 512 in_channels...\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Loaded state_dict from [./crossattn_checkpoints/best_crossattn_offwhite.ckpt]\n","ğŸ“¦ Base Model ë¡œë“œ ì™„ë£Œ: ./crossattn_checkpoints/best_crossattn_offwhite.ckpt\n","making attention of type 'vanilla-xformers' with 512 in_channels\n","building MemoryEfficientAttnBlock with 512 in_channels...\n","Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n","making attention of type 'vanilla-xformers' with 512 in_channels\n","building MemoryEfficientAttnBlock with 512 in_channels...\n","Successfully load new auto-encoder\n","âœ¨ ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ!\n"]}],"source":["# ==========================================\n","# [Cell 1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ëª¨ë¸ ì—”ì§„ ì„¤ì • (ì‹¤í–‰ 1íšŒ)\n","# ==========================================\n","import sys\n","import os\n","import torch\n","import cv2\n","import numpy as np\n","from PIL import Image\n","from omegaconf import OmegaConf\n","from ldm.util import instantiate_from_config\n","from cldm.model import load_state_dict\n","from cldm.ddim_hacked import DDIMSampler\n","import einops\n","\n","# ê²½ë¡œ ì„¤ì • (Colab ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”)\n","BASE_MODEL_PATH = \"./crossattn_checkpoints/best_crossattn_offwhite.ckpt\"\n","CONFIG_PATH = \"./models/cldm_v21_LumiNet.yaml\"\n","\n","# ì „ì—­ ë³€ìˆ˜ (ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ê³„ì† ìœ ì§€í•˜ê¸° ìœ„í•¨)\n","global_model = None\n","global_sampler = None\n","current_adapter_color = None\n","\n","def initialize_engine():\n","    \"\"\"ë² ì´ìŠ¤ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n","    global global_model, global_sampler\n","\n","    if global_model is not None:\n","        print(\"âœ… ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    print(\"ğŸš€ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì‹œì‘... (ìµœì´ˆ 1íšŒ)\")\n","    config = OmegaConf.load(CONFIG_PATH)\n","    model = instantiate_from_config(config.model).cpu()\n","    model.add_new_layers()\n","\n","    if os.path.exists(BASE_MODEL_PATH):\n","        model.load_state_dict(load_state_dict(BASE_MODEL_PATH, location='cpu'), strict=False)\n","        print(f\"ğŸ“¦ Base Model ë¡œë“œ ì™„ë£Œ: {BASE_MODEL_PATH}\")\n","    else:\n","        raise FileNotFoundError(f\"Base Model ì—†ìŒ: {BASE_MODEL_PATH}\")\n","\n","\n","    new_decoder = True #\n","    if new_decoder: #\n","        ae_checkpoint = \"./ckpt/new_decoder.ckpt\" #\n","        model.change_first_stage(ae_checkpoint) #\n","\n","    model.cuda()\n","    model.eval()\n","\n","    global_model = model\n","    global_sampler = DDIMSampler(model)\n","    print(\"âœ¨ ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ!\")\n","\n","def switch_adapter(target_color):\n","    \"\"\"ìš”ì²­ëœ ìƒ‰ìƒìœ¼ë¡œ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ë§Œ êµì²´í•©ë‹ˆë‹¤.\"\"\"\n","    global global_model, current_adapter_color\n","\n","    if current_adapter_color == target_color:\n","        print(f\"â„¹ï¸ ì´ë¯¸ {target_color} ì–´ëŒ‘í„°ê°€ ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    weights_path = f\"./adaptors/adaptor_{target_color}.pth\"\n","\n","    if not os.path.exists(weights_path):\n","        print(f\"(White ëª¨ë“œë¡œ ì§„í–‰)\")\n","        current_adapter_color = \"none\"\n","        return\n","\n","    print(f\"ğŸ”„ ì–´ëŒ‘í„° êµì²´ ì¤‘... ({current_adapter_color} -> {target_color})\")\n","    checkpoint = torch.load(weights_path, map_location='cpu')\n","\n","    # Hot-Swapping (ëª¨ë¸ì„ ë„ì§€ ì•Šê³  ê°€ì¤‘ì¹˜ë§Œ ë®ì–´ì“°ê¸°)\n","    if 'light_encoder' in checkpoint:\n","        global_model.control_model.prior_extracter.model_latents.light_encoder.load_state_dict(checkpoint['light_encoder'])\n","    if 'light_decoder' in checkpoint:\n","        global_model.control_model.prior_extracter.light_decoder.load_state_dict(checkpoint['light_decoder'])\n","\n","    current_adapter_color = target_color\n","    print(f\"âœ… {target_color} ì–´ëŒ‘í„° ì¥ì°© ì™„ë£Œ!\")\n","\n","def run_process(inference_root, color, guidance_scale=9.0, ddim_steps=50):\n","    \"\"\"ì‹¤ì œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜\"\"\"\n","    # 1. ì–´ëŒ‘í„° êµì²´ ì‹œë„\n","    switch_adapter(color)\n","    new_decoder = True #\n","\n","    print(f\"\\nğŸ“‚ ì²˜ë¦¬ ì‹œì‘: {inference_root} (Color: {color})\")\n","\n","    for root, dirs, files in os.walk(inference_root):\n","        off_name = next((f for f in files if 'off' in f.lower()), None)\n","\n","        if off_name:\n","            off_path = os.path.join(root, off_name)\n","            save_path = os.path.join(root, f\"output_{color}.jpg\")\n","            print(f\"â–¶ Processing: {os.path.basename(root)}\")\n","\n","            # ì´ë¯¸ì§€ ë¡œë“œ & ì „ì²˜ë¦¬\n","            img_off = Image.open(off_path).convert(\"RGB\")\n","            orig_w, orig_h = img_off.size\n","            img_off_resized = img_off.resize((512, 512), Image.BICUBIC)\n","            img_ref_resized = Image.new(\"RGB\", (512, 512), (255, 255, 255))\n","\n","            t_off = torch.from_numpy(np.array(img_off_resized).astype(np.float32)/255.0).permute(2,0,1).unsqueeze(0).cuda()\n","            t_ref = torch.from_numpy(np.array(img_ref_resized).astype(np.float32)/255.0).permute(2,0,1).unsqueeze(0).cuda()\n","            hint = torch.cat((t_off, t_ref), dim=1)\n","\n","\n","            input_image_full = cv2.imread(off_path)[..., ::-1] / 255.0 #\n","            inp = cv2.resize(input_image_full, (512, 512)) #\n","            input_img = torch.from_numpy(inp.copy()).float().cuda() #\n","            input_img = input_img.unsqueeze(0) #\n","            input_img = einops.rearrange(input_img, 'b h w c -> b c h w').clone() #\n","\n","            # ì¶”ë¡ \n","            with torch.no_grad():\n","                c_cat = hint\n","                c = global_model.get_unconditional_conditioning(c_cat.shape[0])\n","                cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n","                shape = (4, 512 // 8, 512 // 8)\n","\n","                samples, _ = global_sampler.sample(ddim_steps, 1, shape, cond, verbose=False,\n","                                            unconditional_guidance_scale=guidance_scale)\n","\n","\n","                if new_decoder: #\n","                    ae_hs = global_model.encode_first_stage(input_img * 2 - 1)[1] #\n","                    x_sample = global_model.decode_new_first_stage(samples, ae_hs) #\n","                else: #\n","                    x_sample = global_model.decode_first_stage(samples) #\n","                x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0) #\n","\n","                result_numpy = x_sample.cpu().permute(0, 2, 3, 1).numpy()[0] * 255\n","                result_numpy = result_numpy.astype(np.uint8)\n","                result_final = cv2.resize(result_numpy, (orig_w, orig_h), interpolation=cv2.INTER_LANCZOS4)\n","\n","                cv2.imwrite(save_path, cv2.cvtColor(result_final, cv2.COLOR_RGB2BGR))\n","                print(f\"  ğŸ’¾ Saved: {save_path}\")\n","\n","# ì´ˆê¸°í™” ì‹¤í–‰\n","initialize_engine()"]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] ì¶”ë¡  ì‹¤í–‰ (ë°˜ë³µ ì‹¤í–‰ ê°€ëŠ¥)\n","# ==========================================\n","\n","# 1. ì›í•˜ëŠ” ìƒ‰ìƒê³¼ ì„¤ì • ì…ë ¥\n","TARGET_COLOR = 'white'  # red, green, blue ë“± ë³€ê²½\n","INFERENCE_PATH = \"./images/42\"\n","\n","# 2. ì‹¤í–‰ í•¨ìˆ˜ í˜¸ì¶œ\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"id":"oVEQoY9IdHhW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b2ae1552-4bc8-4463-d700-5068bbddc3ad","executionInfo":{"status":"ok","timestamp":1765206345578,"user_tz":-540,"elapsed":12755,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}},"collapsed":true},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["(White ëª¨ë“œë¡œ ì§„í–‰)\n","\n","ğŸ“‚ ì²˜ë¦¬ ì‹œì‘: ./images/42 (Color: white)\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/output_white.jpg\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/42/output_white.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] ì¶”ë¡  ì‹¤í–‰ (ë°˜ë³µ ì‹¤í–‰ ê°€ëŠ¥)\n","# ==========================================\n","\n","# 1. ì›í•˜ëŠ” ìƒ‰ìƒê³¼ ì„¤ì • ì…ë ¥\n","TARGET_COLOR = 'red'  # red, green, blue ë“± ë³€ê²½\n","INFERENCE_PATH = \"./images/42\"\n","\n","# 2. ì‹¤í–‰ í•¨ìˆ˜ í˜¸ì¶œ\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4wrnGJaFLex","executionInfo":{"status":"ok","timestamp":1765206358381,"user_tz":-540,"elapsed":12799,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}},"outputId":"5d5e14eb-a18e-4e0a-c999-aaebbef35fe8"},"execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ ì–´ëŒ‘í„° êµì²´ ì¤‘... (none -> red)\n","âœ… red ì–´ëŒ‘í„° ì¥ì°© ì™„ë£Œ!\n","\n","ğŸ“‚ ì²˜ë¦¬ ì‹œì‘: ./images/42 (Color: red)\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/output_red.jpg\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/42/output_red.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] ì¶”ë¡  ì‹¤í–‰ (ë°˜ë³µ ì‹¤í–‰ ê°€ëŠ¥)\n","# ==========================================\n","\n","# 1. ì›í•˜ëŠ” ìƒ‰ìƒê³¼ ì„¤ì • ì…ë ¥\n","TARGET_COLOR = 'orange'  # red, green, blue ë“± ë³€ê²½\n","INFERENCE_PATH = \"./images/42\"\n","\n","# 2. ì‹¤í–‰ í•¨ìˆ˜ í˜¸ì¶œ\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cOG_V0DUFL3A","executionInfo":{"status":"ok","timestamp":1765206371264,"user_tz":-540,"elapsed":12868,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}},"outputId":"a6e956e2-9bd9-4259-b254-3d850b5048ca"},"execution_count":112,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ ì–´ëŒ‘í„° êµì²´ ì¤‘... (red -> orange)\n","âœ… orange ì–´ëŒ‘í„° ì¥ì°© ì™„ë£Œ!\n","\n","ğŸ“‚ ì²˜ë¦¬ ì‹œì‘: ./images/42 (Color: orange)\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/output_orange.jpg\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/42/output_orange.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] ì¶”ë¡  ì‹¤í–‰ (ë°˜ë³µ ì‹¤í–‰ ê°€ëŠ¥)\n","# ==========================================\n","\n","# 1. ì›í•˜ëŠ” ìƒ‰ìƒê³¼ ì„¤ì • ì…ë ¥\n","TARGET_COLOR = 'yellow'  # red, green, blue ë“± ë³€ê²½\n","INFERENCE_PATH = \"./images/42\"\n","\n","# 2. ì‹¤í–‰ í•¨ìˆ˜ í˜¸ì¶œ\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsDvpxeuFMah","executionInfo":{"status":"ok","timestamp":1765206384020,"user_tz":-540,"elapsed":12745,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}},"outputId":"1dc755d3-546a-4ffe-f84c-39f9f2b7b8d0"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ ì–´ëŒ‘í„° êµì²´ ì¤‘... (orange -> yellow)\n","âœ… yellow ì–´ëŒ‘í„° ì¥ì°© ì™„ë£Œ!\n","\n","ğŸ“‚ ì²˜ë¦¬ ì‹œì‘: ./images/42 (Color: yellow)\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/output_yellow.jpg\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/42/output_yellow.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] ì¶”ë¡  ì‹¤í–‰ (ë°˜ë³µ ì‹¤í–‰ ê°€ëŠ¥)\n","# ==========================================\n","\n","# 1. ì›í•˜ëŠ” ìƒ‰ìƒê³¼ ì„¤ì • ì…ë ¥\n","TARGET_COLOR = 'green'  # red, green, blue ë“± ë³€ê²½\n","INFERENCE_PATH = \"./images/42\"\n","\n","# 2. ì‹¤í–‰ í•¨ìˆ˜ í˜¸ì¶œ\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BYqPYwGjFMzy","executionInfo":{"status":"ok","timestamp":1765206397081,"user_tz":-540,"elapsed":13044,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}},"outputId":"a6191751-dc40-4c4a-a547-031d5fa89ae6"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ ì–´ëŒ‘í„° êµì²´ ì¤‘... (yellow -> green)\n","âœ… green ì–´ëŒ‘í„° ì¥ì°© ì™„ë£Œ!\n","\n","ğŸ“‚ ì²˜ë¦¬ ì‹œì‘: ./images/42 (Color: green)\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/output_green.jpg\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/42/output_green.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] ì¶”ë¡  ì‹¤í–‰ (ë°˜ë³µ ì‹¤í–‰ ê°€ëŠ¥)\n","# ==========================================\n","\n","# 1. ì›í•˜ëŠ” ìƒ‰ìƒê³¼ ì„¤ì • ì…ë ¥\n","TARGET_COLOR = 'blue'  # red, green, blue ë“± ë³€ê²½\n","INFERENCE_PATH = \"./images/42\"\n","\n","# 2. ì‹¤í–‰ í•¨ìˆ˜ í˜¸ì¶œ\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14Fq4MV1FNLP","executionInfo":{"status":"ok","timestamp":1765206409979,"user_tz":-540,"elapsed":12894,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}},"outputId":"f129e5dd-6748-4931-c4e7-e30eaf176aa3"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ ì–´ëŒ‘í„° êµì²´ ì¤‘... (green -> blue)\n","âœ… blue ì–´ëŒ‘í„° ì¥ì°© ì™„ë£Œ!\n","\n","ğŸ“‚ ì²˜ë¦¬ ì‹œì‘: ./images/42 (Color: blue)\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/output_blue.jpg\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/42/output_blue.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] ì¶”ë¡  ì‹¤í–‰ (ë°˜ë³µ ì‹¤í–‰ ê°€ëŠ¥)\n","# ==========================================\n","\n","# 1. ì›í•˜ëŠ” ìƒ‰ìƒê³¼ ì„¤ì • ì…ë ¥\n","TARGET_COLOR = 'purple'  # red, green, blue ë“± ë³€ê²½\n","INFERENCE_PATH = \"./images/42\"\n","\n","# 2. ì‹¤í–‰ í•¨ìˆ˜ í˜¸ì¶œ\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RIYRM4uEFNlz","executionInfo":{"status":"ok","timestamp":1765206422728,"user_tz":-540,"elapsed":12740,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}},"outputId":"bf9f7652-599f-45d3-b717-be1db004f153"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ ì–´ëŒ‘í„° êµì²´ ì¤‘... (blue -> purple)\n","âœ… purple ì–´ëŒ‘í„° ì¥ì°© ì™„ë£Œ!\n","\n","ğŸ“‚ ì²˜ë¦¬ ì‹œì‘: ./images/42 (Color: purple)\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/output_purple.jpg\n","â–¶ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Saved: ./images/42/42/output_purple.jpg\n"]}]},{"cell_type":"code","source":["# !cp -r \"/content/drive/MyDrive/LumiNet_Files/LumiNet-choi/modi_vae\" \"/content/drive/MyDrive/LumiNet_Files/LumiNet-lee/modi_vae\"\n"],"metadata":{"id":"dA947fbhQ9sN","executionInfo":{"status":"ok","timestamp":1765206422731,"user_tz":-540,"elapsed":1,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["# !cp \"/content/drive/MyDrive/LumiNet_Files/LumiNet-choi/Load_model.py\" \"/content/drive/MyDrive/LumiNet_Files/LumiNet-lee/\""],"metadata":{"id":"CurFhMLGWyPP","executionInfo":{"status":"ok","timestamp":1765206422748,"user_tz":-540,"elapsed":1,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i6jZgs_BYdhn","executionInfo":{"status":"ok","timestamp":1765206422752,"user_tz":-540,"elapsed":1,"user":{"displayName":"ì§€ëŠ¥ì •ë³´ SWì•„ì¹´ë°ë¯¸5ì¡°","userId":"16997830524223997531"}}},"execution_count":118,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"mount_file_id":"1YMSt7_ou-jbXxTfDqd4uakV0DZ32nQwg","authorship_tag":"ABX9TyM3Pxex/5AZmTFXGRqoBMoH"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}