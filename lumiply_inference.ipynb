{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ed115ed6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765260509095,
     "user_tz": -540,
     "elapsed": 8,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     }
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765260510318,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     },
     "user_tz": -540
    },
    "id": "yWtRWFoiLvVV",
    "outputId": "cdd502ea-388b-472a-dd3a-c7a35ba5e993"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/LumiNet_Files/lumiply-colab\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/LumiNet_Files/lumiply-colab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 152697,
     "status": "ok",
     "timestamp": 1765260663997,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     },
     "user_tz": -540
    },
    "id": "OMYjtM_bLyVT",
    "outputId": "a65d7388-7340-4072-d24b-3bc42fe28d5e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting open-clip-torch==2.0.1 (from -r requirements.txt (line 1))\n",
      "  Downloading open_clip_torch-2.0.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.9.0+cu126)\n",
      "Collecting positional-encodings (from -r requirements.txt (line 3))\n",
      "  Downloading positional_encodings-6.0.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (4.12.0.88)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.8.1)\n",
      "Requirement already satisfied: OmegaConf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.3.0)\n",
      "Collecting torchviz (from -r requirements.txt (line 7))\n",
      "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pytorch-lightning==1.5.0 (from -r requirements.txt (line 8))\n",
      "  Downloading pytorch_lightning-1.5.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\n",
      "Collecting xformers==0.0.32.post2 (from -r requirements.txt (line 11))\n",
      "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting share (from -r requirements.txt (line 12))\n",
      "  Downloading share-1.0.4.tar.gz (5.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==2.0.1->-r requirements.txt (line 1)) (0.24.0+cu126)\n",
      "Collecting ftfy (from open-clip-torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==2.0.1->-r requirements.txt (line 1)) (2025.11.3)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==2.0.1->-r requirements.txt (line 1)) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2.0.2)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (6.0.3)\n",
      "Requirement already satisfied: fsspec!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2025.3.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2.19.0)\n",
      "Collecting torchmetrics>=0.4.1 (from pytorch-lightning==1.5.0->-r requirements.txt (line 8))\n",
      "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pyDeprecate==0.3.1 (from pytorch-lightning==1.5.0->-r requirements.txt (line 8))\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (25.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (4.15.0)\n",
      "Collecting torch (from -r requirements.txt (line 2))\n",
      "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.20.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (9.10.2.21)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (0.7.1)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from OmegaConf->-r requirements.txt (line 6)) (4.9.3)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchviz->-r requirements.txt (line 7)) (0.21)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 9)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 9)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch==2.0.1->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (3.10)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (5.29.5)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (3.1.4)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics>=0.4.1->pytorch-lightning==1.5.0->-r requirements.txt (line 8))\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open-clip-torch==2.0.1->-r requirements.txt (line 1)) (0.2.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (2025.11.12)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from open-clip-torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "  Downloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open-clip-torch==2.0.1->-r requirements.txt (line 1)) (11.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.22.0)\n",
      "Downloading open_clip_torch-2.0.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-1.5.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m145.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading positional_encodings-6.0.4-py3-none-any.whl (7.7 kB)\n",
      "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Building wheels for collected packages: share\n",
      "  Building wheel for share (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for share: filename=share-1.0.4-py3-none-any.whl size=8962 sha256=33f098c7b0614b884d177b4834dccec86aa8daaf5353b32a53e1388bf296483a\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/2a/ef/7bdb887ffe1c617bd5085e127a77b1720719f6b729d596acb7\n",
      "Successfully built share\n",
      "Installing collected packages: share, triton, pyDeprecate, positional-encodings, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, ftfy, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, xformers, torchviz, torchvision, torchmetrics, pytorch-lightning, open-clip-torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.0+cu126\n",
      "    Uninstalling torch-2.9.0+cu126:\n",
      "      Successfully uninstalled torch-2.9.0+cu126\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.24.0+cu126\n",
      "    Uninstalling torchvision-0.24.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.24.0+cu126\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed ftfy-6.3.1 lightning-utilities-0.15.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 open-clip-torch-2.0.1 positional-encodings-6.0.4 pyDeprecate-0.3.1 pytorch-lightning-1.5.0 share-1.0.4 torch-2.8.0 torchmetrics-1.8.2 torchvision-0.23.0 torchviz-0.0.3 triton-3.4.0 xformers-0.0.32.post2\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
      "Collecting flask-cors\n",
      "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.3.1)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
      "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
      "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pyngrok, flask-cors\n",
      "Successfully installed flask-cors-6.0.1 pyngrok-7.5.0\n"
     ]
    }
   ],
   "source": [
    "# LumiNet 패키지 설치\n",
    "!pip install -r requirements.txt\n",
    "# Flask 패키지 설치\n",
    "!pip install flask flask-cors requests pillow pyngrok torchvision flask_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "6eab6a62731441289e6380524abbd2a5",
      "e72f00d9811b4d9b9603017e092bd050",
      "f19abb2a113b420095ba7cd7d364e4a8",
      "effc719a3e4e4f15a024a78d87c8b08c",
      "8bfb93b5ba3340b39ab16b53fc0bd407",
      "34b7a4ddd86e4efd98364278b2eb279e",
      "b948f60ae3364e1db243e4f3d355ab02",
      "dd6a282d85c346e9a381d7c2909bd68e",
      "ce41acd1c9c14490a58b83b955314c0c",
      "8446f732557c4a9d8aae141c5d053e67",
      "efdd65fb8ff14c45abb5e4967745908d",
      "4fd38459e24f4fadb38a57b73691fab5",
      "36a62f8e5dab4d4f80f3a0d52e2244a3",
      "9b4ff49702044ecfa1f77f7a522b8206",
      "94b45ccab9684a4393ce682a27196f16",
      "4d5a7caa012e493d9d964bfdaa3e3347",
      "b87609890de742ee819d2e3b40d6ac37",
      "3438da02f3684677ae27702d0905a422",
      "73c891d1686843519a095ac04465c1ed",
      "ca09518b8c7e4de6bce047978e629e95"
     ]
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1765260668640,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     },
     "user_tz": -540
    },
    "id": "aUxy3_XcQKsR",
    "outputId": "5f746a70-40f1-4a8b-dc00-d153d84cfe0a"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6eab6a62731441289e6380524abbd2a5"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jfMUIOSS8noQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765260674438,
     "user_tz": -540,
     "elapsed": 217,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     }
    }
   },
   "outputs": [],
   "source": [
    "!lsof -i | grep :5000 # 아무것도 안 떠야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFu9p_P18s3J"
   },
   "outputs": [],
   "source": [
    "# !kill -9 38800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c493595075154a7f8274f9c43a932664",
      "50599ec4fc2a47e180ddf05605ce0d73",
      "45a50a4b2456491fb926982039821ebe",
      "9376b9c781d74eb799d7d6296378eaf3",
      "8027fc6bc477408fb0aed0726f90428b",
      "0a2c3f87fe854d5da4cfce05f53e0a5e",
      "746f5f966d8d45b3949bd305ed67c8b7",
      "59de8ca4ae3c4f9ea47b15daf5c6d04b",
      "d138412d0c4a431ba02e45a566321f5b",
      "7590829dac0b4a369520fb27728db214",
      "d0bd4e711d9d47fb9c6c339e01f13c2c"
     ]
    },
    "executionInfo": {
     "elapsed": 417457,
     "status": "ok",
     "timestamp": 1765261093413,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     },
     "user_tz": -540
    },
    "id": "pzDH8NyOKRUE",
    "outputId": "4763f2c3-e0fa-4bf8-e9a2-1b5a36b5cbbf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "베이스 모델 로딩 시작... (최초 1회)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/core/lightning.py:2058: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ControlLDM: Running in v-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "DiffusionWrapper has 1042.99 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c493595075154a7f8274f9c43a932664"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
      "Loaded state_dict from [./ckpt/trained_crossattn.ckpt]\n",
      "Base Model 로드 완료: ./ckpt/trained_crossattn.ckpt\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Successfully load new auto-encoder\n",
      "엔진 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [Cell 1] 모델/엔진 초기화 (약 5~10분 소요)\n",
    "# =========================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from cldm.model import load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "import einops\n",
    "\n",
    "# 경로 설정 (Colab 경로에 맞게 수정 필요)\n",
    "# BASE_MODEL_PATH = \"./crossattn_checkpoints/best_crossattn_offwhite.ckpt\"\n",
    "BASE_MODEL_PATH = \"./ckpt/trained_crossattn.ckpt\"\n",
    "CONFIG_PATH = \"./models/cldm_v21_LumiNet.yaml\"\n",
    "\n",
    "# 전역 변수 (모델을 메모리에 계속 유지하기 위함)\n",
    "global_model = None\n",
    "global_sampler = None\n",
    "current_adapter_color = None\n",
    "\n",
    "def initialize_engine():\n",
    "    \"\"\"베이스 모델을 메모리에 로드합니다.\"\"\"\n",
    "    global global_model, global_sampler\n",
    "\n",
    "    if global_model is not None:\n",
    "        print(\"모델이 이미 로드되어 있습니다.\")\n",
    "        return\n",
    "\n",
    "    print(\"베이스 모델 로딩 시작... (최초 1회)\")\n",
    "    config = OmegaConf.load(CONFIG_PATH)\n",
    "    model = instantiate_from_config(config.model).cpu()\n",
    "    model.add_new_layers()\n",
    "\n",
    "    if os.path.exists(BASE_MODEL_PATH):\n",
    "        model.load_state_dict(load_state_dict(BASE_MODEL_PATH, location='cpu'), strict=False)\n",
    "        print(f\"Base Model 로드 완료: {BASE_MODEL_PATH}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Base Model 없음: {BASE_MODEL_PATH}\")\n",
    "\n",
    "    new_decoder = True #\n",
    "    if new_decoder: #\n",
    "        ae_checkpoint = \"./ckpt/new_decoder.ckpt\" #\n",
    "        model.change_first_stage(ae_checkpoint) #\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    global_model = model\n",
    "    global_sampler = DDIMSampler(model)\n",
    "    print(\"엔진 준비 완료!\")\n",
    "\n",
    "def switch_adapter(target_color):\n",
    "    \"\"\"요청된 색상으로 어댑터 가중치만 교체합니다.\"\"\"\n",
    "    global global_model, current_adapter_color\n",
    "\n",
    "    if current_adapter_color == target_color:\n",
    "        print(f\"이미 {target_color} 어댑터가 적용되어 있습니다.\")\n",
    "        return\n",
    "\n",
    "    weights_path = f\"./adaptors/adaptor_{target_color}.pth\"\n",
    "\n",
    "    if not os.path.exists(weights_path):\n",
    "        print(f\"(White 모드로 진행)\")\n",
    "        current_adapter_color = \"none\"\n",
    "        return\n",
    "\n",
    "    print(f\"어댑터 교체 중... ({current_adapter_color} -> {target_color})\")\n",
    "    checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "\n",
    "    # Hot-Swapping (모델을 끄지 않고 가중치만 덮어쓰기)\n",
    "    if 'light_encoder' in checkpoint:\n",
    "        global_model.control_model.prior_extracter.model_latents.light_encoder.load_state_dict(checkpoint['light_encoder'])\n",
    "    if 'light_decoder' in checkpoint:\n",
    "        global_model.control_model.prior_extracter.light_decoder.load_state_dict(checkpoint['light_decoder'])\n",
    "\n",
    "    # ==================================================\n",
    "    # 정밀 검증 (Verification)\n",
    "    print(\"\\n🔍 가중치 적용 상태 검증 중...\")\n",
    "    is_valid = True\n",
    "\n",
    "    # Encoder 검증\n",
    "    if 'light_encoder' in checkpoint:\n",
    "        key = list(checkpoint['light_encoder'].keys())[0]\n",
    "\n",
    "        # [수정] 비교 전에 둘 다 CPU로 이동\n",
    "        file_val = checkpoint['light_encoder'][key].cpu()\n",
    "        model_val = global_model.control_model.prior_extracter.model_latents.light_encoder.state_dict()[key].cpu()\n",
    "\n",
    "        if torch.allclose(file_val, model_val, atol=1e-6):\n",
    "            print(f\"Encoder 검증 통과! (Key: {key})\")\n",
    "        else:\n",
    "            print(f\"Encoder 검증 실패! 값이 다릅니다.\")\n",
    "            is_valid = False\n",
    "\n",
    "    # Adaptor 검증\n",
    "    if 'light_decoder' in checkpoint:\n",
    "        key = list(checkpoint['light_decoder'].keys())[0]\n",
    "\n",
    "        # [수정] 비교 전에 둘 다 CPU로 이동\n",
    "        file_val = checkpoint['light_decoder'][key].cpu()\n",
    "        model_val = global_model.control_model.prior_extracter.light_decoder.state_dict()[key].cpu()\n",
    "\n",
    "        if torch.allclose(file_val, model_val, atol=1e-6):\n",
    "            print(f\"Adaptor 검증 통과! (Key: {key})\")\n",
    "        else:\n",
    "            print(f\"Adaptor 검증 실패! 값이 다릅니다.\")\n",
    "            is_valid = False\n",
    "\n",
    "    if not is_valid:\n",
    "        print(\"경고: 가중치가 제대로 적용되지 않았을 수 있습니다. 코드를 확인하세요.\")\n",
    "    else:\n",
    "        print(\"모든 가중치가 완벽하게 적용되었습니다. (Color Mode ON)\")\n",
    "    # ==================================================\n",
    "\n",
    "    current_adapter_color = target_color\n",
    "    print(f\"{target_color} 어댑터 장착 완료\")\n",
    "\n",
    "def run_inference_single_image(off_path, color, guidance_scale=9.0, ddim_steps=50):\n",
    "    \"\"\"\n",
    "    off_path: 합성된 'off' 이미지 경로 (원본 해상도)\n",
    "    color: 'red', 'orange', 'yellow', 'green', 'blue', 'purple' 등\n",
    "    반환: 저장된 결과 이미지 경로\n",
    "    \"\"\"\n",
    "    switch_adapter(color)\n",
    "    new_decoder = True #\n",
    "\n",
    "    print(f\"\\nSingle Image 처리 시작: {off_path} (Color: {color})\")\n",
    "\n",
    "    img_off = Image.open(off_path).convert(\"RGB\")\n",
    "    orig_w, orig_h = img_off.size\n",
    "    img_off_resized = img_off.resize((512, 512), Image.BICUBIC)\n",
    "    img_ref_resized = Image.new(\"RGB\", (512, 512), (255, 255, 255))\n",
    "\n",
    "    t_off = torch.from_numpy(np.array(img_off_resized).astype(np.float32) / 255.0).permute(2,0,1).unsqueeze(0).cuda()\n",
    "    t_ref = torch.from_numpy(np.array(img_ref_resized).astype(np.float32) / 255.0).permute(2,0,1).unsqueeze(0).cuda()\n",
    "\n",
    "    hint = torch.cat((t_off, t_ref), dim=1)\n",
    "\n",
    "    input_image_full = cv2.imread(off_path)[..., ::-1] / 255.0 #\n",
    "    inp = cv2.resize(input_image_full, (512, 512)) #\n",
    "    input_img = torch.from_numpy(inp.copy()).float().cuda() #\n",
    "    input_img = input_img.unsqueeze(0) #\n",
    "    input_img = einops.rearrange(input_img, 'b h w c -> b c h w').clone() #\n",
    "\n",
    "    # 추론\n",
    "    with torch.no_grad():\n",
    "        c_cat = hint\n",
    "        c = global_model.get_unconditional_conditioning(c_cat.shape[0])\n",
    "        cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n",
    "        shape = (4, 512 // 8, 512 // 8)\n",
    "\n",
    "        samples, _ = global_sampler.sample(\n",
    "            ddim_steps, 1, shape, cond, verbose=False,\n",
    "            unconditional_guidance_scale=guidance_scale\n",
    "        )\n",
    "\n",
    "        # x_sample = global_model.decode_first_stage(samples)\n",
    "        # x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "        if new_decoder: #\n",
    "            ae_hs = global_model.encode_first_stage(input_img * 2 - 1)[1] #\n",
    "            x_sample = global_model.decode_new_first_stage(samples, ae_hs) #\n",
    "        else: #\n",
    "            x_sample = global_model.decode_first_stage(samples) #\n",
    "        x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0) #\n",
    "\n",
    "        result_numpy = x_sample.cpu().permute(0, 2, 3, 1).numpy()[0] * 255\n",
    "        result_numpy = result_numpy.astype(np.uint8)\n",
    "        result_final = cv2.resize(result_numpy, (orig_w, orig_h), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "        # 저장 경로는 off_path 기준으로 output_{color}.jpg\n",
    "        root, fname = os.path.split(off_path)\n",
    "        save_path = os.path.join(root, f\"output_{color}.jpg\")\n",
    "        cv2.imwrite(save_path, cv2.cvtColor(result_final, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "# 기존 run_process(inference_root, color)는 여러 파일 batch 용으로 남겨두고,\n",
    "# 온라인 요청용으로는 run_inference_single_image(...)를 핵심으로 사용.\n",
    "\n",
    "initialize_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AW0ckY3iLt_C",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765261113565,
     "user_tz": -540,
     "elapsed": 220,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     }
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# [Cell 2] Flask + ngrok 서버\n",
    "# =========================\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "INFERENCE_ROOT = \"./images/inference\"\n",
    "os.makedirs(INFERENCE_ROOT, exist_ok=True)\n",
    "\n",
    "PUBLIC_URL = None  # [Cell 3] 에서 ngrok.connect 후 채움\n",
    "\n",
    "@app.before_request\n",
    "def handle_ngrok_warning():\n",
    "    \"\"\"ngrok 브라우저 경고 페이지 우회\"\"\"\n",
    "    if request.headers.get(\"ngrok-skip-browser-warning\"):\n",
    "        pass\n",
    "\n",
    "@app.before_request\n",
    "def log_request_info():\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"요청 수신: {request.method} {request.path}\")\n",
    "    print(f\"Headers: {dict(request.headers)}\")\n",
    "    if request.method == \"POST\":\n",
    "        print(f\"Form data keys: {list(request.form.keys())}\")\n",
    "        print(f\"Files keys: {list(request.files.keys())}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "@app.route(\"/health\", methods=[\"GET\"])\n",
    "def health_check():\n",
    "    print(\"Health check 요청 수신\")\n",
    "    return jsonify(\n",
    "        {\n",
    "            \"status\": \"healthy\",\n",
    "            \"message\": \"Colab 서버가 실행 중입니다.\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "    ), 200\n",
    "\n",
    "@app.route(\"/process\", methods=[\"POST\"])\n",
    "def process_image():\n",
    "    \"\"\"\n",
    "    FastAPI에서 전송된 합성 이미지를 받아,\n",
    "    /images/inference/{job_id}/off.png 로 저장한 뒤,\n",
    "    요청된 단일 색상(color)에 대해서만 output_{color}.jpg 를 생성하고,\n",
    "    해당 색상 결과 + 입력(off) 이미지 URL 을 FastAPI/프론트에 반환.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Process 엔드포인트 호출됨!\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        if \"image\" not in request.files:\n",
    "            print(\"이미지 파일이 없습니다.\")\n",
    "            return jsonify({\"error\": \"이미지 파일이 없습니다.\"}), 400\n",
    "\n",
    "        file = request.files[\"image\"]\n",
    "        job_id = request.form.get(\"job_id\")\n",
    "        callback_url = request.form.get(\"callback_url\")\n",
    "        color_param = request.form.get(\"color\")  # ★ 추가: 단일 색상 지정\n",
    "\n",
    "        print(f\"📦 수신된 데이터:\")\n",
    "        print(f\"   - job_id: {job_id}\")\n",
    "        print(f\"   - callback_url: {callback_url}\")\n",
    "        print(f\"   - color: {color_param}\")\n",
    "        print(f\"   - 파일명: {file.filename}\")\n",
    "        print(f\"   - 파일 타입: {file.content_type}\")\n",
    "\n",
    "        if not job_id:\n",
    "            print(\"job_id가 없습니다.\")\n",
    "            return jsonify({\"error\": \"job_id가 없습니다.\"}), 400\n",
    "\n",
    "        # 지원 색상 검증 (단일 색상만 허용)\n",
    "        base_colors = [\"white\", \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\"]\n",
    "        if not color_param:\n",
    "            return jsonify({\"error\": \"color 파라미터가 필요합니다.\"}), 400\n",
    "        if color_param not in base_colors:\n",
    "            return jsonify({\"error\": f\"지원하지 않는 색상입니다: {color_param}\"}), 400\n",
    "\n",
    "        color_key = color_param\n",
    "\n",
    "        if not callback_url:\n",
    "            print(\"callback_url이 없습니다. 콜백을 전송하지 않습니다.\")\n",
    "\n",
    "        print(f\"[{job_id}] 이미지 처리 시작\")\n",
    "\n",
    "        # 1) job 전용 디렉토리 생성\n",
    "        job_dir = os.path.join(INFERENCE_ROOT, job_id)\n",
    "        os.makedirs(job_dir, exist_ok=True)\n",
    "\n",
    "        # 2) 입력 이미지를 off.png 로 저장\n",
    "        off_path = os.path.join(job_dir, \"off.png\")\n",
    "        image_bytes = file.read()\n",
    "        with open(off_path, \"wb\") as f:\n",
    "            f.write(image_bytes)\n",
    "\n",
    "        print(f\"[{job_id}] 입력 이미지 저장 완료: {off_path}\")\n",
    "        print(f\"[{job_id}] 이미지 크기: {len(image_bytes)} bytes\")\n",
    "\n",
    "        from pathlib import Path\n",
    "\n",
    "        images = {}\n",
    "        # for ck in color_key:\n",
    "        print(f\"[{job_id}] === {color_key} 색상 처리 시작 ===\")\n",
    "        output_path = run_inference_single_image(off_path=off_path, color=color_key)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            rel_path = f\"/static/inference/{job_id}/{Path(output_path).name}\"\n",
    "            images[color_key] = f\"{PUBLIC_URL}{rel_path}\"\n",
    "            print(f\"[{job_id}] {color_key} 결과 URL: {images[color_key]}\")\n",
    "        else:\n",
    "            print(f\"[{job_id}] {color_key} 결과 이미지가 존재하지 않습니다: {output_path}\")\n",
    "\n",
    "        # 인풋(off) 이미지 URL\n",
    "        input_rel = f\"/static/inference/{job_id}/off.png\"\n",
    "        input_image_url = f\"{PUBLIC_URL}{input_rel}\"\n",
    "\n",
    "        response_data = {\n",
    "            \"job_id\": job_id,\n",
    "            \"status\": \"completed\",\n",
    "            \"result\": {\n",
    "                \"images\": images,\n",
    "                \"input_image_url\": input_image_url,\n",
    "            },\n",
    "            \"message\": \"색상 이미지 생성 완료\",\n",
    "        }\n",
    "\n",
    "        # 콜백\n",
    "        if callback_url:\n",
    "            try:\n",
    "                print(f\"[{job_id}] FastAPI로 결과 전송 중...\")\n",
    "                callback_response = requests.post(\n",
    "                    callback_url,\n",
    "                    json=response_data,\n",
    "                    timeout=60,\n",
    "                    headers={\"ngrok-skip-browser-warning\": \"true\"},\n",
    "                )\n",
    "                print(f\"[{job_id}] 콜백 응답 상태: {callback_response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[{job_id}] 콜백 전송 오류: {str(e)}\")\n",
    "\n",
    "        print(f\"[{job_id}] 처리 완료\")\n",
    "        return jsonify(response_data), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"처리 오류: {error_msg}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"success\": False, \"error\": error_msg}), 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6649,
     "status": "ok",
     "timestamp": 1765261121166,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     },
     "user_tz": -540
    },
    "id": "CNa9tq4IMuF5",
    "outputId": "9743b4e6-1783-4cef-fa81-a039f9047eea"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Flask 서버 시작... (0.0.0.0:5000)\n",
      "Flask 서버 스레드 실행 중\n",
      "==================================================\n",
      "✅ Colab 서버가 시작되었습니다!\n",
      "🌐 Public URL: https://undented-preinflectional-madeline.ngrok-free.dev\n",
      "📝 FastAPI의 COLAB_WEBHOOK_URL 환경 변수에 다음 URL을 설정하세요:\n",
      "   https://undented-preinflectional-madeline.ngrok-free.dev/process\n",
      "==================================================\n",
      "\n",
      "💡 복사할 URL:\n",
      "   https://undented-preinflectional-madeline.ngrok-free.dev/process\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [Cell 3] ngrok + Flask 서버 (백그라운드) 설정\n",
    "# =========================\n",
    "\n",
    "from pyngrok import ngrok\n",
    "from werkzeug.middleware.shared_data import SharedDataMiddleware\n",
    "from werkzeug.serving import make_server\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# 1) 정적 파일 서빙 설정 (Flask app은 이미 Cell 2에서 정의되어 있다고 가정)\n",
    "app.wsgi_app = SharedDataMiddleware(app.wsgi_app, {\n",
    "    \"/static/inference\": INFERENCE_ROOT,\n",
    "})\n",
    "\n",
    "# 2) Flask 서버를 백그라운드 스레드에서 돌리기 위한 클래스\n",
    "class FlaskServerThread(threading.Thread):\n",
    "    def __init__(self, app, host=\"0.0.0.0\", port=5000):\n",
    "        super().__init__()\n",
    "        self.app = app\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.server = None\n",
    "        self.ctx = None\n",
    "        self.daemon = True  # 노트북 종료 시 같이 죽도록\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"Flask 서버 시작... ({self.host}:{self.port})\")\n",
    "        # WSGI 서버 인스턴스를 직접 잡아둔다\n",
    "        self.server = make_server(self.host, self.port, self.app)\n",
    "        self.ctx = self.app.app_context()\n",
    "        self.ctx.push()\n",
    "        try:\n",
    "            self.server.serve_forever()\n",
    "        finally:\n",
    "            self.server.server_close()\n",
    "            self.ctx.pop()\n",
    "            print(\"Flask 서버 종료 완료\")\n",
    "\n",
    "    def shutdown(self):\n",
    "        if self.server:\n",
    "            print(\"Flask 서버 shutdown 요청\")\n",
    "            self.server.shutdown()  # serve_forever 루프 깨기\n",
    "\n",
    "# 3) 이미 열려 있는 ngrok 터널이 있으면 정리\n",
    "if \"NGROK_TUNNEL\" in globals() and NGROK_TUNNEL is not None:\n",
    "    try:\n",
    "        print(\"기존 ngrok 터널 닫는 중:\", NGROK_TUNNEL.public_url)\n",
    "        NGROK_TUNNEL.close()\n",
    "    except Exception as e:\n",
    "        print(\"기존 터널 close 중 오류:\", e)\n",
    "    NGROK_TUNNEL = None\n",
    "\n",
    "# 4) Flask 서버 스레드 시작 (백그라운드)\n",
    "flask_server = FlaskServerThread(app, host=\"0.0.0.0\", port=5000)\n",
    "flask_server.start()\n",
    "\n",
    "# 서버가 뜰 시간 약간 대기\n",
    "time.sleep(2)\n",
    "print(\"Flask 서버 스레드 실행 중\")\n",
    "\n",
    "# 5) ngrok 설정\n",
    "NGROK_TOKEN = \"NGROK_TOKEN_입력\"\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "ngrok.kill()  # 혹시 남아 있을 수 있는 ngrok 프로세스 정리\n",
    "\n",
    "# 새 터널 생성 (Flask가 5000 포트에서 이미 LISTEN 중이어야 함)\n",
    "NGROK_TUNNEL = ngrok.connect(5000)\n",
    "\n",
    "public_url = getattr(NGROK_TUNNEL, \"public_url\", str(NGROK_TUNNEL))\n",
    "\n",
    "if public_url.startswith(\"http://\"):\n",
    "    public_url = public_url.replace(\"http://\", \"https://\")\n",
    "elif not public_url.startswith(\"https://\"):\n",
    "    public_url = f\"https://{public_url}\"\n",
    "\n",
    "PUBLIC_URL = public_url\n",
    "process_url = f\"{public_url}/process\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Colab 서버가 시작되었습니다!\")\n",
    "print(f\"🌐 Public URL: {public_url}\")\n",
    "print(\"📝 FastAPI의 COLAB_WEBHOOK_URL 환경 변수에 다음 URL을 설정하세요:\")\n",
    "print(f\"   {process_url}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n💡 복사할 URL:\")\n",
    "print(f\"   {process_url}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# [Stop Cell] ngrok 터널 완전히 종료\n",
    "\n",
    "# from pyngrok import ngrok\n",
    "\n",
    "# try:\n",
    "#     # 우리가 저장해 둔 터널 객체가 있다면 먼저 닫기\n",
    "#     if \"NGROK_TUNNEL\" in globals() and NGROK_TUNNEL is not None:\n",
    "#         try:\n",
    "#             NGROK_TUNNEL.close()\n",
    "#             print(\"NGROK_TUNNEL.close() 호출 완료\")\n",
    "#         except Exception as e:\n",
    "#             print(\"NGROK_TUNNEL.close() 중 오류:\", e)\n",
    "#         NGROK_TUNNEL = None\n",
    "# except NameError:\n",
    "#     # NGROK_TUNNEL 자체가 없을 수도 있음\n",
    "#     pass\n",
    "\n",
    "# # 로컬 ngrok 프로세스 통째로 종료\n",
    "# try:\n",
    "#     ngrok.kill()\n",
    "#     print(\"ngrok.kill() 호출 완료 (모든 터널 정리)\")\n",
    "# except Exception as e:\n",
    "#     print(\"ngrok.kill() 중 오류:\", e)\n",
    "\n",
    "flask_server.shutdown()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Qi_K23yLNTI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765262452430,
     "user_tz": -540,
     "elapsed": 215,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     }
    },
    "outputId": "d79a2d09-cc49-48fe-b3f9-106b444a2845"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Flask 서버 shutdown 요청\n",
      "Flask 서버 종료 완료\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 테스트 코드"
   ],
   "metadata": {
    "id": "qGurvgoHJ_a6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pwd"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fa94SmVTEUve",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765207885310,
     "user_tz": -540,
     "elapsed": 107,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     }
    },
    "outputId": "1853e8f7-f908-4c9d-f9b2-a179faeba1ed"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/LumiNet_Files/LumiNet-lee\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141547,
     "status": "ok",
     "timestamp": 1765208027785,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     },
     "user_tz": -540
    },
    "id": "PDPYyaGQ7XaV",
    "outputId": "d252463c-4f64-492f-bfd3-a168f1edd489"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================================================\n",
      "Sending request for color: white\n",
      "==================================================\n",
      "요청 수신: POST /process\n",
      "Headers: {'Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'User-Agent': 'python-requests/2.32.4', 'Content-Length': '717510', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Content-Type': 'multipart/form-data; boundary=ca8e0f2a1c067cae162ce6be65f4e061', 'Ngrok-Skip-Browser-Warning': 'true', 'X-Forwarded-For': '34.126.159.241', 'X-Forwarded-Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'X-Forwarded-Proto': 'https'}\n",
      "Form data keys: ['job_id', 'callback_url', 'color']\n",
      "Files keys: ['image']\n",
      "==================================================\n",
      "==================================================\n",
      "Process 엔드포인트 호출됨!\n",
      "==================================================\n",
      "📦 수신된 데이터:\n",
      "   - job_id: debug-job\n",
      "   - callback_url: \n",
      "   - color: white\n",
      "   - 파일명: 23_off.jpeg\n",
      "   - 파일 타입: None\n",
      "callback_url이 없습니다. 콜백을 전송하지 않습니다.\n",
      "[debug-job] 이미지 처리 시작\n",
      "[debug-job] 입력 이미지 저장 완료: ./images/inference/debug-job/off.png\n",
      "[debug-job] 이미지 크기: 717082 bytes\n",
      "[debug-job] === white 색상 처리 시작 ===\n",
      "(White 모드로 진행)\n",
      "\n",
      "Single Image 처리 시작: ./images/inference/debug-job/off.png (Color: white)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  7.65it/s]\n",
      "INFO:werkzeug:127.0.0.1 - - [08/Dec/2025 15:31:40] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: ./images/inference/debug-job/output_white.jpg\n",
      "[debug-job] white 결과 URL: https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_white.jpg\n",
      "[debug-job] 처리 완료\n",
      "Status: 200\n",
      "Response: {\"job_id\":\"debug-job\",\"message\":\"\\uc0c9\\uc0c1 \\uc774\\ubbf8\\uc9c0 \\uc0dd\\uc131 \\uc644\\ub8cc\",\"result\":{\"images\":{\"white\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_white.jpg\"},\"input_image_url\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/off.png\"},\"status\":\"completed\"}\n",
      "\n",
      "==================================================\n",
      "Sending request for color: red\n",
      "==================================================\n",
      "요청 수신: POST /process\n",
      "Headers: {'Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'User-Agent': 'python-requests/2.32.4', 'Content-Length': '717508', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Content-Type': 'multipart/form-data; boundary=26c2959e32540c703eadf1d8e73488d1', 'Ngrok-Skip-Browser-Warning': 'true', 'X-Forwarded-For': '34.126.159.241', 'X-Forwarded-Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'X-Forwarded-Proto': 'https'}\n",
      "Form data keys: ['job_id', 'callback_url', 'color']\n",
      "Files keys: ['image']\n",
      "==================================================\n",
      "==================================================\n",
      "Process 엔드포인트 호출됨!\n",
      "==================================================\n",
      "📦 수신된 데이터:\n",
      "   - job_id: debug-job\n",
      "   - callback_url: \n",
      "   - color: red\n",
      "   - 파일명: 23_off.jpeg\n",
      "   - 파일 타입: None\n",
      "callback_url이 없습니다. 콜백을 전송하지 않습니다.\n",
      "[debug-job] 이미지 처리 시작\n",
      "[debug-job] 입력 이미지 저장 완료: ./images/inference/debug-job/off.png\n",
      "[debug-job] 이미지 크기: 717082 bytes\n",
      "[debug-job] === red 색상 처리 시작 ===\n",
      "어댑터 교체 중... (none -> red)\n",
      "\n",
      "🔍 가중치 적용 상태 검증 중...\n",
      "Encoder 검증 통과! (Key: 0.weight)\n",
      "Adaptor 검증 통과! (Key: 0.weight)\n",
      "모든 가중치가 완벽하게 적용되었습니다. (Color Mode ON)\n",
      "red 어댑터 장착 완료\n",
      "\n",
      "Single Image 처리 시작: ./images/inference/debug-job/off.png (Color: red)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  8.96it/s]\n",
      "INFO:werkzeug:127.0.0.1 - - [08/Dec/2025 15:31:59] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: ./images/inference/debug-job/output_red.jpg\n",
      "[debug-job] red 결과 URL: https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_red.jpg\n",
      "[debug-job] 처리 완료\n",
      "Status: 200\n",
      "Response: {\"job_id\":\"debug-job\",\"message\":\"\\uc0c9\\uc0c1 \\uc774\\ubbf8\\uc9c0 \\uc0dd\\uc131 \\uc644\\ub8cc\",\"result\":{\"images\":{\"red\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_red.jpg\"},\"input_image_url\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/off.png\"},\"status\":\"completed\"}\n",
      "\n",
      "==================================================\n",
      "Sending request for color: orange\n",
      "==================================================\n",
      "요청 수신: POST /process\n",
      "Headers: {'Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'User-Agent': 'python-requests/2.32.4', 'Content-Length': '717511', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Content-Type': 'multipart/form-data; boundary=834074b8b21b54950d4e6d92a2137a9c', 'Ngrok-Skip-Browser-Warning': 'true', 'X-Forwarded-For': '34.126.159.241', 'X-Forwarded-Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'X-Forwarded-Proto': 'https'}\n",
      "Form data keys: ['job_id', 'callback_url', 'color']\n",
      "Files keys: ['image']\n",
      "==================================================\n",
      "==================================================\n",
      "Process 엔드포인트 호출됨!\n",
      "==================================================\n",
      "📦 수신된 데이터:\n",
      "   - job_id: debug-job\n",
      "   - callback_url: \n",
      "   - color: orange\n",
      "   - 파일명: 23_off.jpeg\n",
      "   - 파일 타입: None\n",
      "callback_url이 없습니다. 콜백을 전송하지 않습니다.\n",
      "[debug-job] 이미지 처리 시작\n",
      "[debug-job] 입력 이미지 저장 완료: ./images/inference/debug-job/off.png\n",
      "[debug-job] 이미지 크기: 717082 bytes\n",
      "[debug-job] === orange 색상 처리 시작 ===\n",
      "어댑터 교체 중... (red -> orange)\n",
      "\n",
      "🔍 가중치 적용 상태 검증 중...\n",
      "Encoder 검증 통과! (Key: 0.weight)\n",
      "Adaptor 검증 통과! (Key: 0.weight)\n",
      "모든 가중치가 완벽하게 적용되었습니다. (Color Mode ON)\n",
      "orange 어댑터 장착 완료\n",
      "\n",
      "Single Image 처리 시작: ./images/inference/debug-job/off.png (Color: orange)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  8.96it/s]\n",
      "INFO:werkzeug:127.0.0.1 - - [08/Dec/2025 15:32:24] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: ./images/inference/debug-job/output_orange.jpg\n",
      "[debug-job] orange 결과 URL: https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_orange.jpg\n",
      "[debug-job] 처리 완료\n",
      "Status: 200\n",
      "Response: {\"job_id\":\"debug-job\",\"message\":\"\\uc0c9\\uc0c1 \\uc774\\ubbf8\\uc9c0 \\uc0dd\\uc131 \\uc644\\ub8cc\",\"result\":{\"images\":{\"orange\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_orange.jpg\"},\"input_image_url\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/off.png\"},\"status\":\"completed\"}\n",
      "\n",
      "==================================================\n",
      "Sending request for color: yellow\n",
      "==================================================\n",
      "요청 수신: POST /process\n",
      "Headers: {'Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'User-Agent': 'python-requests/2.32.4', 'Content-Length': '717511', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Content-Type': 'multipart/form-data; boundary=c54fb788c94dfea7323b2b56fb01bf47', 'Ngrok-Skip-Browser-Warning': 'true', 'X-Forwarded-For': '34.126.159.241', 'X-Forwarded-Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'X-Forwarded-Proto': 'https'}\n",
      "Form data keys: ['job_id', 'callback_url', 'color']\n",
      "Files keys: ['image']\n",
      "==================================================\n",
      "==================================================\n",
      "Process 엔드포인트 호출됨!\n",
      "==================================================\n",
      "📦 수신된 데이터:\n",
      "   - job_id: debug-job\n",
      "   - callback_url: \n",
      "   - color: yellow\n",
      "   - 파일명: 23_off.jpeg\n",
      "   - 파일 타입: None\n",
      "callback_url이 없습니다. 콜백을 전송하지 않습니다.\n",
      "[debug-job] 이미지 처리 시작\n",
      "[debug-job] 입력 이미지 저장 완료: ./images/inference/debug-job/off.png\n",
      "[debug-job] 이미지 크기: 717082 bytes\n",
      "[debug-job] === yellow 색상 처리 시작 ===\n",
      "어댑터 교체 중... (orange -> yellow)\n",
      "\n",
      "🔍 가중치 적용 상태 검증 중...\n",
      "Encoder 검증 통과! (Key: 0.weight)\n",
      "Adaptor 검증 통과! (Key: 0.weight)\n",
      "모든 가중치가 완벽하게 적용되었습니다. (Color Mode ON)\n",
      "yellow 어댑터 장착 완료\n",
      "\n",
      "Single Image 처리 시작: ./images/inference/debug-job/off.png (Color: yellow)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  8.94it/s]\n",
      "INFO:werkzeug:127.0.0.1 - - [08/Dec/2025 15:32:38] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: ./images/inference/debug-job/output_yellow.jpg\n",
      "[debug-job] yellow 결과 URL: https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_yellow.jpg\n",
      "[debug-job] 처리 완료\n",
      "Status: 200\n",
      "Response: {\"job_id\":\"debug-job\",\"message\":\"\\uc0c9\\uc0c1 \\uc774\\ubbf8\\uc9c0 \\uc0dd\\uc131 \\uc644\\ub8cc\",\"result\":{\"images\":{\"yellow\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_yellow.jpg\"},\"input_image_url\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/off.png\"},\"status\":\"completed\"}\n",
      "\n",
      "==================================================\n",
      "Sending request for color: green\n",
      "==================================================\n",
      "요청 수신: POST /process\n",
      "Headers: {'Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'User-Agent': 'python-requests/2.32.4', 'Content-Length': '717510', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Content-Type': 'multipart/form-data; boundary=4187ad76fa0cb76ce99e5ded92a1394b', 'Ngrok-Skip-Browser-Warning': 'true', 'X-Forwarded-For': '34.126.159.241', 'X-Forwarded-Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'X-Forwarded-Proto': 'https'}\n",
      "Form data keys: ['job_id', 'callback_url', 'color']\n",
      "Files keys: ['image']\n",
      "==================================================\n",
      "==================================================\n",
      "Process 엔드포인트 호출됨!\n",
      "==================================================\n",
      "📦 수신된 데이터:\n",
      "   - job_id: debug-job\n",
      "   - callback_url: \n",
      "   - color: green\n",
      "   - 파일명: 23_off.jpeg\n",
      "   - 파일 타입: None\n",
      "callback_url이 없습니다. 콜백을 전송하지 않습니다.\n",
      "[debug-job] 이미지 처리 시작\n",
      "[debug-job] 입력 이미지 저장 완료: ./images/inference/debug-job/off.png\n",
      "[debug-job] 이미지 크기: 717082 bytes\n",
      "[debug-job] === green 색상 처리 시작 ===\n",
      "어댑터 교체 중... (yellow -> green)\n",
      "\n",
      "🔍 가중치 적용 상태 검증 중...\n",
      "Encoder 검증 통과! (Key: 0.weight)\n",
      "Adaptor 검증 통과! (Key: 0.weight)\n",
      "모든 가중치가 완벽하게 적용되었습니다. (Color Mode ON)\n",
      "green 어댑터 장착 완료\n",
      "\n",
      "Single Image 처리 시작: ./images/inference/debug-job/off.png (Color: green)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  9.00it/s]\n",
      "INFO:werkzeug:127.0.0.1 - - [08/Dec/2025 15:32:59] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: ./images/inference/debug-job/output_green.jpg\n",
      "[debug-job] green 결과 URL: https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_green.jpg\n",
      "[debug-job] 처리 완료\n",
      "Status: 200\n",
      "Response: {\"job_id\":\"debug-job\",\"message\":\"\\uc0c9\\uc0c1 \\uc774\\ubbf8\\uc9c0 \\uc0dd\\uc131 \\uc644\\ub8cc\",\"result\":{\"images\":{\"green\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_green.jpg\"},\"input_image_url\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/off.png\"},\"status\":\"completed\"}\n",
      "\n",
      "==================================================\n",
      "Sending request for color: blue\n",
      "==================================================\n",
      "요청 수신: POST /process\n",
      "Headers: {'Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'User-Agent': 'python-requests/2.32.4', 'Content-Length': '717509', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Content-Type': 'multipart/form-data; boundary=17fbda1a60db98ba7f35a9701158e34f', 'Ngrok-Skip-Browser-Warning': 'true', 'X-Forwarded-For': '34.126.159.241', 'X-Forwarded-Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'X-Forwarded-Proto': 'https'}\n",
      "Form data keys: ['job_id', 'callback_url', 'color']\n",
      "Files keys: ['image']\n",
      "==================================================\n",
      "==================================================\n",
      "Process 엔드포인트 호출됨!\n",
      "==================================================\n",
      "📦 수신된 데이터:\n",
      "   - job_id: debug-job\n",
      "   - callback_url: \n",
      "   - color: blue\n",
      "   - 파일명: 23_off.jpeg\n",
      "   - 파일 타입: None\n",
      "callback_url이 없습니다. 콜백을 전송하지 않습니다.\n",
      "[debug-job] 이미지 처리 시작\n",
      "[debug-job] 입력 이미지 저장 완료: ./images/inference/debug-job/off.png\n",
      "[debug-job] 이미지 크기: 717082 bytes\n",
      "[debug-job] === blue 색상 처리 시작 ===\n",
      "어댑터 교체 중... (green -> blue)\n",
      "\n",
      "🔍 가중치 적용 상태 검증 중...\n",
      "Encoder 검증 통과! (Key: 0.weight)\n",
      "Adaptor 검증 통과! (Key: 0.weight)\n",
      "모든 가중치가 완벽하게 적용되었습니다. (Color Mode ON)\n",
      "blue 어댑터 장착 완료\n",
      "\n",
      "Single Image 처리 시작: ./images/inference/debug-job/off.png (Color: blue)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  8.93it/s]\n",
      "INFO:werkzeug:127.0.0.1 - - [08/Dec/2025 15:33:23] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: ./images/inference/debug-job/output_blue.jpg\n",
      "[debug-job] blue 결과 URL: https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_blue.jpg\n",
      "[debug-job] 처리 완료\n",
      "Status: 200\n",
      "Response: {\"job_id\":\"debug-job\",\"message\":\"\\uc0c9\\uc0c1 \\uc774\\ubbf8\\uc9c0 \\uc0dd\\uc131 \\uc644\\ub8cc\",\"result\":{\"images\":{\"blue\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_blue.jpg\"},\"input_image_url\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/off.png\"},\"status\":\"completed\"}\n",
      "\n",
      "==================================================\n",
      "Sending request for color: purple\n",
      "==================================================\n",
      "요청 수신: POST /process\n",
      "Headers: {'Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'User-Agent': 'python-requests/2.32.4', 'Content-Length': '717511', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Content-Type': 'multipart/form-data; boundary=70370bd63a99bc476bd25a34e94218a8', 'Ngrok-Skip-Browser-Warning': 'true', 'X-Forwarded-For': '34.126.159.241', 'X-Forwarded-Host': 'undented-preinflectional-madeline.ngrok-free.dev', 'X-Forwarded-Proto': 'https'}\n",
      "Form data keys: ['job_id', 'callback_url', 'color']\n",
      "Files keys: ['image']\n",
      "==================================================\n",
      "==================================================\n",
      "Process 엔드포인트 호출됨!\n",
      "==================================================\n",
      "📦 수신된 데이터:\n",
      "   - job_id: debug-job\n",
      "   - callback_url: \n",
      "   - color: purple\n",
      "   - 파일명: 23_off.jpeg\n",
      "   - 파일 타입: None\n",
      "callback_url이 없습니다. 콜백을 전송하지 않습니다.\n",
      "[debug-job] 이미지 처리 시작\n",
      "[debug-job] 입력 이미지 저장 완료: ./images/inference/debug-job/off.png\n",
      "[debug-job] 이미지 크기: 717082 bytes\n",
      "[debug-job] === purple 색상 처리 시작 ===\n",
      "어댑터 교체 중... (blue -> purple)\n",
      "\n",
      "🔍 가중치 적용 상태 검증 중...\n",
      "Encoder 검증 통과! (Key: 0.weight)\n",
      "Adaptor 검증 통과! (Key: 0.weight)\n",
      "모든 가중치가 완벽하게 적용되었습니다. (Color Mode ON)\n",
      "purple 어댑터 장착 완료\n",
      "\n",
      "Single Image 처리 시작: ./images/inference/debug-job/off.png (Color: purple)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  8.86it/s]\n",
      "INFO:werkzeug:127.0.0.1 - - [08/Dec/2025 15:33:47] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: ./images/inference/debug-job/output_purple.jpg\n",
      "[debug-job] purple 결과 URL: https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_purple.jpg\n",
      "[debug-job] 처리 완료\n",
      "Status: 200\n",
      "Response: {\"job_id\":\"debug-job\",\"message\":\"\\uc0c9\\uc0c1 \\uc774\\ubbf8\\uc9c0 \\uc0dd\\uc131 \\uc644\\ub8cc\",\"result\":{\"images\":{\"purple\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/output_purple.jpg\"},\"input_image_url\":\"https://undented-preinflectional-madeline.ngrok-free.dev/static/inference/debug-job/off.png\"},\"status\":\"completed\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "colors = [\"white\", \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\"]\n",
    "job_id = \"debug-job\"\n",
    "\n",
    "for color in colors:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Sending request for color: {color}\")\n",
    "\n",
    "    with open(\"./23_off.jpeg\", \"rb\") as f:\n",
    "        resp = requests.post(\n",
    "            f\"{PUBLIC_URL}/process\",\n",
    "            # files={\"image\": (\"off.png\", f, \"image/png\")},\n",
    "            files={\"image\": f},\n",
    "            data={\"job_id\": job_id, \"callback_url\": \"\", \"color\": color},\n",
    "            headers={\"ngrok-skip-browser-warning\": \"true\"},\n",
    "            timeout=300,\n",
    "        )\n",
    "\n",
    "    print(\"Status:\", resp.status_code)\n",
    "    print(\"Response:\", resp.text[:400])  # 너무 길어지지 않게 앞부분만 출력"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ckpt = torch.load(\"./adaptors/adaptor_red.pth\", map_location=\"cpu\")\n",
    "# print(\"checkpoint top-level keys:\", ckpt.keys())\n",
    "\n",
    "# if \"light_encoder\" in ckpt:\n",
    "#     print(\"light_encoder state_dict keys:\", list(ckpt[\"light_encoder\"].keys())[:5])\n",
    "# else:\n",
    "#     print(\"❗ light_encoder 키가 없습니다. 저장 형식이 다릅니다.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVdwELRG9vQ7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765170672738,
     "user_tz": -540,
     "elapsed": 170,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     }
    },
    "outputId": "a004cbf2-57c8-46b0-9e77-0c380d178f07"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "checkpoint top-level keys: dict_keys(['light_encoder', 'light_decoder'])\n",
      "light_encoder state_dict keys: ['0.weight', '0.bias', '2.weight', '2.bias']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "\n",
    "# ckpt_red = torch.load(\"./adaptors/adaptor_red.pth\", map_location=\"cpu\")\n",
    "# ckpt_blue = torch.load(\"./adaptors/adaptor_blue.pth\", map_location=\"cpu\")\n",
    "\n",
    "# le_red = ckpt_red[\"light_encoder\"]\n",
    "# le_blue = ckpt_blue[\"light_encoder\"]\n",
    "\n",
    "# ld_red = ckpt_red[\"light_decoder\"]\n",
    "# ld_blue = ckpt_blue[\"light_decoder\"]\n",
    "\n",
    "# le_same = True\n",
    "# ld_same = True\n",
    "# for k in le_red.keys():\n",
    "#     if not torch.allclose(le_red[k], le_blue[k]):\n",
    "#         print(\"차이 나는 encoder key:\", k, \"max diff:\", (le_red[k] - le_blue[k]).abs().max().item())\n",
    "#         le_same = False\n",
    "#         break\n",
    "#     if not torch.allclose(ld_red[k], ld_blue[k]):\n",
    "#         print(\"차이 나는 decoder key:\", k, \"max diff:\", (ld_red[k] - ld_blue[k]).abs().max().item())\n",
    "#         ld_same = False\n",
    "#         break\n",
    "\n",
    "# print(\"레드/블루 encoder 완전 동일?\", le_same)\n",
    "# print(\"레드/블루 decoder 완전 동일?\", ld_same)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xwhst3bxXvLb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765171115930,
     "user_tz": -540,
     "elapsed": 457,
     "user": {
      "displayName": "지능정보 SW아카데미5조",
      "userId": "16997830524223997531"
     }
    },
    "outputId": "5a5101de-a68e-4e40-e3c6-07aee2ad3b6a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "차이 나는 decoder key: 0.weight max diff: 0.0059593068435788155\n",
      "레드/블루 encoder 완전 동일? True\n",
      "레드/블루 decoder 완전 동일? False\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "FF_H6JlANLPd"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "mount_file_id": "1mPVGzn39ItpG22nuyN3rF9yd2rB_jh5K",
   "authorship_tag": "ABX9TyNca/XgP/hBHsBh9J+akiKk"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}