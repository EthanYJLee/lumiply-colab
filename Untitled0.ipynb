{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","mount_file_id":"15cconSgdNSNW-GUHtWp1g0MS-u5HXB0a","authorship_tag":"ABX9TyNs6Ma0d8aBc9yDMpierhcB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b2634860cafe4c2fa47ade7fd82ff876":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9617a15643b44c4785ffd29a2a645aad","IPY_MODEL_9df2192c272d43dda6896b0f73ed7e04","IPY_MODEL_74c8a13516e14c45a417279601024e28"],"layout":"IPY_MODEL_5cd2f1b77938499ebd3427115cdb1135"}},"9617a15643b44c4785ffd29a2a645aad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a88313feec4424881a8f2b1e10ea68a","placeholder":"‚Äã","style":"IPY_MODEL_42ae336331ce41bca6ba16d9df61fa9b","value":"open_clip_pytorch_model.bin:‚Äá100%"}},"9df2192c272d43dda6896b0f73ed7e04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2771ce847ad4b0c9a1dadda0483b752","max":3944692325,"min":0,"orientation":"horizontal","style":"IPY_MODEL_01c436066301470b98af1b7ec97287d0","value":3944692325}},"74c8a13516e14c45a417279601024e28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7513aedce1314fc097c1b26ac67a4b91","placeholder":"‚Äã","style":"IPY_MODEL_d2db7a94966b42d793d2073b1d5ef302","value":"‚Äá3.94G/3.94G‚Äá[00:09&lt;00:00,‚Äá810MB/s]"}},"5cd2f1b77938499ebd3427115cdb1135":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a88313feec4424881a8f2b1e10ea68a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42ae336331ce41bca6ba16d9df61fa9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2771ce847ad4b0c9a1dadda0483b752":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01c436066301470b98af1b7ec97287d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7513aedce1314fc097c1b26ac67a4b91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2db7a94966b42d793d2073b1d5ef302":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"WPU1Eh3sW7yF","executionInfo":{"status":"ok","timestamp":1765206710190,"user_tz":-540,"elapsed":10,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}}},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/LumiNet_Files/LumiNet-lee/"],"metadata":{"id":"9LcvKzM6W-Pz","executionInfo":{"status":"ok","timestamp":1765206710856,"user_tz":-540,"elapsed":663,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c8378c8-744b-47e3-bc67-08c6f8f73551"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/LumiNet_Files/LumiNet-lee\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"id":"xO3DFw3JW_Mq","executionInfo":{"status":"ok","timestamp":1765206856295,"user_tz":-540,"elapsed":145437,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"12dc29d4-da03-4176-da05-6a5397d163fe"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting open-clip-torch==2.0.1 (from -r requirements.txt (line 1))\n","  Downloading open_clip_torch-2.0.1-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.9.0+cu126)\n","Collecting positional-encodings (from -r requirements.txt (line 3))\n","  Downloading positional_encodings-6.0.4-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (4.12.0.88)\n","Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.8.1)\n","Requirement already satisfied: OmegaConf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.3.0)\n","Collecting torchviz (from -r requirements.txt (line 7))\n","  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n","Collecting pytorch-lightning==1.5.0 (from -r requirements.txt (line 8))\n","  Downloading pytorch_lightning-1.5.0-py3-none-any.whl.metadata (31 kB)\n","Requirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.57.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\n","Collecting xformers==0.0.32.post2 (from -r requirements.txt (line 11))\n","  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n","Collecting share (from -r requirements.txt (line 12))\n","  Downloading share-1.0.4.tar.gz (5.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==2.0.1->-r requirements.txt (line 1)) (0.24.0+cu126)\n","Collecting ftfy (from open-clip-torch==2.0.1->-r requirements.txt (line 1))\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==2.0.1->-r requirements.txt (line 1)) (2025.11.3)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==2.0.1->-r requirements.txt (line 1)) (0.36.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2.0.2)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.0.0)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (6.0.3)\n","Requirement already satisfied: fsspec!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2025.3.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2.19.0)\n","Collecting torchmetrics>=0.4.1 (from pytorch-lightning==1.5.0->-r requirements.txt (line 8))\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Collecting pyDeprecate==0.3.1 (from pytorch-lightning==1.5.0->-r requirements.txt (line 8))\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (25.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (4.15.0)\n","Collecting torch (from -r requirements.txt (line 2))\n","  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (9.10.2.21)\n","Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-curand-cu12==10.3.9.90 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (0.7.1)\n","Collecting nvidia-nccl-cu12==2.27.3 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.8.90 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==3.4.0 (from torch->-r requirements.txt (line 2))\n","  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from OmegaConf->-r requirements.txt (line 6)) (4.9.3)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchviz->-r requirements.txt (line 7)) (0.21)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 9)) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 9)) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->-r requirements.txt (line 9)) (0.7.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (3.13.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch==2.0.1->-r requirements.txt (line 1)) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.76.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (3.10)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (5.29.5)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (3.1.3)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics>=0.4.1->pytorch-lightning==1.5.0->-r requirements.txt (line 8))\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open-clip-torch==2.0.1->-r requirements.txt (line 1)) (0.2.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0->-r requirements.txt (line 9)) (2025.11.12)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision (from open-clip-torch==2.0.1->-r requirements.txt (line 1))\n","  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n","  Downloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n","  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open-clip-torch==2.0.1->-r requirements.txt (line 1)) (11.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.0->-r requirements.txt (line 8)) (1.22.0)\n","Downloading open_clip_torch-2.0.1-py3-none-any.whl (1.4 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytorch_lightning-1.5.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m151.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading positional_encodings-6.0.4-py3-none-any.whl (7.7 kB)\n","Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n","Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m131.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Building wheels for collected packages: share\n","  Building wheel for share (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for share: filename=share-1.0.4-py3-none-any.whl size=8962 sha256=84d73222239226c21db943c5587680e8afd83fcb48036aecaf60249154170bd2\n","  Stored in directory: /root/.cache/pip/wheels/e7/2a/ef/7bdb887ffe1c617bd5085e127a77b1720719f6b729d596acb7\n","Successfully built share\n","Installing collected packages: share, triton, pyDeprecate, positional-encodings, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, ftfy, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, xformers, torchviz, torchvision, torchmetrics, pytorch-lightning, open-clip-torch\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.5.0\n","    Uninstalling triton-3.5.0:\n","      Successfully uninstalled triton-3.5.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n","    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.27.5\n","    Uninstalling nvidia-nccl-cu12-2.27.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufile-cu12\n","    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n","    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n","      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.9.0+cu126\n","    Uninstalling torch-2.9.0+cu126:\n","      Successfully uninstalled torch-2.9.0+cu126\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.24.0+cu126\n","    Uninstalling torchvision-0.24.0+cu126:\n","      Successfully uninstalled torchvision-0.24.0+cu126\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed ftfy-6.3.1 lightning-utilities-0.15.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 open-clip-torch-2.0.1 positional-encodings-6.0.4 pyDeprecate-0.3.1 pytorch-lightning-1.5.0 share-1.0.4 torch-2.8.0 torchmetrics-1.8.2 torchvision-0.23.0 torchviz-0.0.3 triton-3.4.0 xformers-0.0.32.post2\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 1] ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏ Î∞è Î™®Îç∏ ÏóîÏßÑ ÏÑ§Ï†ï (Ïã§Ìñâ 1Ìöå)\n","# ==========================================\n","import sys\n","import os\n","import torch\n","import cv2\n","import numpy as np\n","from PIL import Image\n","from omegaconf import OmegaConf\n","from ldm.util import instantiate_from_config\n","from cldm.model import load_state_dict\n","from cldm.ddim_hacked import DDIMSampler\n","import einops\n","\n","# Í≤ΩÎ°ú ÏÑ§Ï†ï (Colab Í≤ΩÎ°úÏóê ÎßûÍ≤å ÏàòÏ†ï ÌïÑÏöî)\n","BASE_MODEL_PATH = \"./crossattn_checkpoints/best_crossattn_offwhite.ckpt\"\n","CONFIG_PATH = \"./models/cldm_v21_LumiNet.yaml\"\n","\n","# Ï†ÑÏó≠ Î≥ÄÏàò (Î™®Îç∏ÏùÑ Î©îÎ™®Î¶¨Ïóê Í≥ÑÏÜç Ïú†ÏßÄÌïòÍ∏∞ ÏúÑÌï®)\n","global_model = None\n","global_sampler = None\n","current_adapter_color = None\n","\n","def initialize_engine():\n","    \"\"\"Î≤†Ïù¥Ïä§ Î™®Îç∏ÏùÑ Î©îÎ™®Î¶¨Ïóê Î°úÎìúÌï©ÎãàÎã§.\"\"\"\n","    global global_model, global_sampler\n","\n","    if global_model is not None:\n","        print(\"‚úÖ Î™®Îç∏Ïù¥ Ïù¥ÎØ∏ Î°úÎìúÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\")\n","        return\n","\n","    print(\"üöÄ Î≤†Ïù¥Ïä§ Î™®Îç∏ Î°úÎî© ÏãúÏûë... (ÏµúÏ¥à 1Ìöå)\")\n","    config = OmegaConf.load(CONFIG_PATH)\n","    model = instantiate_from_config(config.model).cpu()\n","    model.add_new_layers()\n","\n","    if os.path.exists(BASE_MODEL_PATH):\n","        model.load_state_dict(load_state_dict(BASE_MODEL_PATH, location='cpu'), strict=False)\n","        print(f\"üì¶ Base Model Î°úÎìú ÏôÑÎ£å: {BASE_MODEL_PATH}\")\n","    else:\n","        raise FileNotFoundError(f\"Base Model ÏóÜÏùå: {BASE_MODEL_PATH}\")\n","\n","\n","    new_decoder = True #\n","    if new_decoder: #\n","        ae_checkpoint = \"./ckpt/new_decoder.ckpt\" #\n","        model.change_first_stage(ae_checkpoint) #\n","\n","    model.cuda()\n","    model.eval()\n","\n","    global_model = model\n","    global_sampler = DDIMSampler(model)\n","    print(\"‚ú® ÏóîÏßÑ Ï§ÄÎπÑ ÏôÑÎ£å!\")\n","\n","def switch_adapter(target_color):\n","    \"\"\"ÏöîÏ≤≠Îêú ÏÉâÏÉÅÏúºÎ°ú Ïñ¥ÎåëÌÑ∞ Í∞ÄÏ§ëÏπòÎßå ÍµêÏ≤¥Ìï©ÎãàÎã§.\"\"\"\n","    global global_model, current_adapter_color\n","\n","    if current_adapter_color == target_color:\n","        print(f\"‚ÑπÔ∏è Ïù¥ÎØ∏ {target_color} Ïñ¥ÎåëÌÑ∞Í∞Ä Ï†ÅÏö©ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\")\n","        return\n","\n","    weights_path = f\"./adaptors/adaptor_{target_color}.pth\"\n","\n","    if not os.path.exists(weights_path):\n","        print(f\"(White Î™®ÎìúÎ°ú ÏßÑÌñâ)\")\n","        current_adapter_color = \"none\"\n","        return\n","\n","    print(f\"üîÑ Ïñ¥ÎåëÌÑ∞ ÍµêÏ≤¥ Ï§ë... ({current_adapter_color} -> {target_color})\")\n","    checkpoint = torch.load(weights_path, map_location='cpu')\n","\n","    # Hot-Swapping (Î™®Îç∏ÏùÑ ÎÅÑÏßÄ ÏïäÍ≥† Í∞ÄÏ§ëÏπòÎßå ÎçÆÏñ¥Ïì∞Í∏∞)\n","    if 'light_encoder' in checkpoint:\n","        global_model.control_model.prior_extracter.model_latents.light_encoder.load_state_dict(checkpoint['light_encoder'])\n","    if 'light_decoder' in checkpoint:\n","        global_model.control_model.prior_extracter.light_decoder.load_state_dict(checkpoint['light_decoder'])\n","\n","    current_adapter_color = target_color\n","    print(f\"‚úÖ {target_color} Ïñ¥ÎåëÌÑ∞ Ïû•Ï∞© ÏôÑÎ£å!\")\n","\n","def run_process(inference_root, color, guidance_scale=9.0, ddim_steps=50):\n","    \"\"\"Ïã§Ï†ú Ï∂îÎ°†ÏùÑ ÏàòÌñâÌïòÎäî Ìï®Ïàò\"\"\"\n","    # 1. Ïñ¥ÎåëÌÑ∞ ÍµêÏ≤¥ ÏãúÎèÑ\n","    switch_adapter(color)\n","    new_decoder = True #\n","\n","    print(f\"\\nüìÇ Ï≤òÎ¶¨ ÏãúÏûë: {inference_root} (Color: {color})\")\n","\n","    for root, dirs, files in os.walk(inference_root):\n","        off_name = next((f for f in files if 'off' in f.lower()), None)\n","\n","        if off_name:\n","            off_path = os.path.join(root, off_name)\n","            save_path = os.path.join(root, f\"output_{color}.jpg\")\n","            print(f\"‚ñ∂ Processing: {os.path.basename(root)}\")\n","\n","            # Ïù¥ÎØ∏ÏßÄ Î°úÎìú & Ï†ÑÏ≤òÎ¶¨\n","            img_off = Image.open(off_path).convert(\"RGB\")\n","            orig_w, orig_h = img_off.size\n","            img_off_resized = img_off.resize((512, 512), Image.BICUBIC)\n","            img_ref_resized = Image.new(\"RGB\", (512, 512), (255, 255, 255))\n","\n","            t_off = torch.from_numpy(np.array(img_off_resized).astype(np.float32)/255.0).permute(2,0,1).unsqueeze(0).cuda()\n","            t_ref = torch.from_numpy(np.array(img_ref_resized).astype(np.float32)/255.0).permute(2,0,1).unsqueeze(0).cuda()\n","            hint = torch.cat((t_off, t_ref), dim=1)\n","\n","\n","            input_image_full = cv2.imread(off_path)[..., ::-1] / 255.0 #\n","            inp = cv2.resize(input_image_full, (512, 512)) #\n","            input_img = torch.from_numpy(inp.copy()).float().cuda() #\n","            input_img = input_img.unsqueeze(0) #\n","            input_img = einops.rearrange(input_img, 'b h w c -> b c h w').clone() #\n","\n","            # Ï∂îÎ°†\n","            with torch.no_grad():\n","                c_cat = hint\n","                c = global_model.get_unconditional_conditioning(c_cat.shape[0])\n","                cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n","                shape = (4, 512 // 8, 512 // 8)\n","\n","                samples, _ = global_sampler.sample(ddim_steps, 1, shape, cond, verbose=False,\n","                                            unconditional_guidance_scale=guidance_scale)\n","\n","\n","                if new_decoder: #\n","                    ae_hs = global_model.encode_first_stage(input_img * 2 - 1)[1] #\n","                    x_sample = global_model.decode_new_first_stage(samples, ae_hs) #\n","                else: #\n","                    x_sample = global_model.decode_first_stage(samples) #\n","                x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0) #\n","\n","                result_numpy = x_sample.cpu().permute(0, 2, 3, 1).numpy()[0] * 255\n","                result_numpy = result_numpy.astype(np.uint8)\n","                result_final = cv2.resize(result_numpy, (orig_w, orig_h), interpolation=cv2.INTER_LANCZOS4)\n","\n","                cv2.imwrite(save_path, cv2.cvtColor(result_final, cv2.COLOR_RGB2BGR))\n","                print(f\"  üíæ Saved: {save_path}\")\n","\n","# Ï¥àÍ∏∞Ìôî Ïã§Ìñâ\n","initialize_engine()"],"metadata":{"id":"2YTYqqB6XA8y","executionInfo":{"status":"ok","timestamp":1765207036823,"user_tz":-540,"elapsed":180523,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b2634860cafe4c2fa47ade7fd82ff876","9617a15643b44c4785ffd29a2a645aad","9df2192c272d43dda6896b0f73ed7e04","74c8a13516e14c45a417279601024e28","5cd2f1b77938499ebd3427115cdb1135","8a88313feec4424881a8f2b1e10ea68a","42ae336331ce41bca6ba16d9df61fa9b","a2771ce847ad4b0c9a1dadda0483b752","01c436066301470b98af1b7ec97287d0","7513aedce1314fc097c1b26ac67a4b91","d2db7a94966b42d793d2073b1d5ef302"]},"outputId":"42ca3cbe-9454-4671-d0e0-2ae54fc46fa8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Î≤†Ïù¥Ïä§ Î™®Îç∏ Î°úÎî© ÏãúÏûë... (ÏµúÏ¥à 1Ìöå)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/pytorch_lightning/core/lightning.py:2058: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n","  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook\n"]},{"output_type":"stream","name":"stdout","text":["ControlLDM: Running in v-prediction mode\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","DiffusionWrapper has 1042.99 M params.\n","making attention of type 'vanilla-xformers' with 512 in_channels\n","building MemoryEfficientAttnBlock with 512 in_channels...\n","Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n","making attention of type 'vanilla-xformers' with 512 in_channels\n","building MemoryEfficientAttnBlock with 512 in_channels...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n","  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n","/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2634860cafe4c2fa47ade7fd82ff876"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n","Loaded state_dict from [./crossattn_checkpoints/best_crossattn_offwhite.ckpt]\n","üì¶ Base Model Î°úÎìú ÏôÑÎ£å: ./crossattn_checkpoints/best_crossattn_offwhite.ckpt\n","making attention of type 'vanilla-xformers' with 512 in_channels\n","building MemoryEfficientAttnBlock with 512 in_channels...\n","Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n","making attention of type 'vanilla-xformers' with 512 in_channels\n","building MemoryEfficientAttnBlock with 512 in_channels...\n","Successfully load new auto-encoder\n","‚ú® ÏóîÏßÑ Ï§ÄÎπÑ ÏôÑÎ£å!\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] Ï∂îÎ°† Ïã§Ìñâ (Î∞òÎ≥µ Ïã§Ìñâ Í∞ÄÎä•)\n","# ==========================================\n","\n","# 1. ÏõêÌïòÎäî ÏÉâÏÉÅÍ≥º ÏÑ§Ï†ï ÏûÖÎ†•\n","TARGET_COLOR = 'white'  # red, green, blue Îì± Î≥ÄÍ≤Ω\n","INFERENCE_PATH = \"./images/42/42\"\n","\n","# 2. Ïã§Ìñâ Ìï®Ïàò Ìò∏Ï∂ú\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"id":"bqF0d5P6XCnU","executionInfo":{"status":"ok","timestamp":1765207045284,"user_tz":-540,"elapsed":8456,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b9ffed1-e62e-48de-c628-3e640f13b5e5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(White Î™®ÎìúÎ°ú ÏßÑÌñâ)\n","\n","üìÇ Ï≤òÎ¶¨ ÏãúÏûë: ./images/42/42 (Color: white)\n","‚ñ∂ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:06<00:00,  7.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  üíæ Saved: ./images/42/42/output_white.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] Ï∂îÎ°† Ïã§Ìñâ (Î∞òÎ≥µ Ïã§Ìñâ Í∞ÄÎä•)\n","# ==========================================\n","\n","# 1. ÏõêÌïòÎäî ÏÉâÏÉÅÍ≥º ÏÑ§Ï†ï ÏûÖÎ†•\n","TARGET_COLOR = 'red'  # red, green, blue Îì± Î≥ÄÍ≤Ω\n","INFERENCE_PATH = \"./images/42/42\"\n","\n","# 2. Ïã§Ìñâ Ìï®Ïàò Ìò∏Ï∂ú\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"id":"YGyPxz0IXcvj","executionInfo":{"status":"ok","timestamp":1765207055735,"user_tz":-540,"elapsed":10448,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d88e99f3-2139-40e3-b3b2-f9a3b3546580"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Ïñ¥ÎåëÌÑ∞ ÍµêÏ≤¥ Ï§ë... (none -> red)\n","‚úÖ red Ïñ¥ÎåëÌÑ∞ Ïû•Ï∞© ÏôÑÎ£å!\n","\n","üìÇ Ï≤òÎ¶¨ ÏãúÏûë: ./images/42/42 (Color: red)\n","‚ñ∂ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:05<00:00,  8.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  üíæ Saved: ./images/42/42/output_red.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] Ï∂îÎ°† Ïã§Ìñâ (Î∞òÎ≥µ Ïã§Ìñâ Í∞ÄÎä•)\n","# ==========================================\n","\n","# 1. ÏõêÌïòÎäî ÏÉâÏÉÅÍ≥º ÏÑ§Ï†ï ÏûÖÎ†•\n","TARGET_COLOR = 'orange'  # red, green, blue Îì± Î≥ÄÍ≤Ω\n","INFERENCE_PATH = \"./images/42/42\"\n","\n","# 2. Ïã§Ìñâ Ìï®Ïàò Ìò∏Ï∂ú\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"id":"nZImh8oIXdKW","executionInfo":{"status":"ok","timestamp":1765207069516,"user_tz":-540,"elapsed":13780,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d6124157-ef77-4662-bca2-5e30dc4f704a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Ïñ¥ÎåëÌÑ∞ ÍµêÏ≤¥ Ï§ë... (red -> orange)\n","‚úÖ orange Ïñ¥ÎåëÌÑ∞ Ïû•Ï∞© ÏôÑÎ£å!\n","\n","üìÇ Ï≤òÎ¶¨ ÏãúÏûë: ./images/42/42 (Color: orange)\n","‚ñ∂ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:05<00:00,  8.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  üíæ Saved: ./images/42/42/output_orange.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] Ï∂îÎ°† Ïã§Ìñâ (Î∞òÎ≥µ Ïã§Ìñâ Í∞ÄÎä•)\n","# ==========================================\n","\n","# 1. ÏõêÌïòÎäî ÏÉâÏÉÅÍ≥º ÏÑ§Ï†ï ÏûÖÎ†•\n","TARGET_COLOR = 'yellow'  # red, green, blue Îì± Î≥ÄÍ≤Ω\n","INFERENCE_PATH = \"./images/42/42\"\n","\n","# 2. Ïã§Ìñâ Ìï®Ïàò Ìò∏Ï∂ú\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"id":"28us5j8fXdeI","executionInfo":{"status":"ok","timestamp":1765207087678,"user_tz":-540,"elapsed":18156,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b24d99c9-a566-4f0e-9d71-067e4a342d61"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Ïñ¥ÎåëÌÑ∞ ÍµêÏ≤¥ Ï§ë... (orange -> yellow)\n","‚úÖ yellow Ïñ¥ÎåëÌÑ∞ Ïû•Ï∞© ÏôÑÎ£å!\n","\n","üìÇ Ï≤òÎ¶¨ ÏãúÏûë: ./images/42/42 (Color: yellow)\n","‚ñ∂ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:05<00:00,  8.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  üíæ Saved: ./images/42/42/output_yellow.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] Ï∂îÎ°† Ïã§Ìñâ (Î∞òÎ≥µ Ïã§Ìñâ Í∞ÄÎä•)\n","# ==========================================\n","\n","# 1. ÏõêÌïòÎäî ÏÉâÏÉÅÍ≥º ÏÑ§Ï†ï ÏûÖÎ†•\n","TARGET_COLOR = 'green'  # red, green, blue Îì± Î≥ÄÍ≤Ω\n","INFERENCE_PATH = \"./images/42/42\"\n","\n","# 2. Ïã§Ìñâ Ìï®Ïàò Ìò∏Ï∂ú\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"id":"C0un1OE7Xdnd","executionInfo":{"status":"ok","timestamp":1765207104133,"user_tz":-540,"elapsed":16452,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5577691e-2f53-48e1-99fc-210a36ea9731"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Ïñ¥ÎåëÌÑ∞ ÍµêÏ≤¥ Ï§ë... (yellow -> green)\n","‚úÖ green Ïñ¥ÎåëÌÑ∞ Ïû•Ï∞© ÏôÑÎ£å!\n","\n","üìÇ Ï≤òÎ¶¨ ÏãúÏûë: ./images/42/42 (Color: green)\n","‚ñ∂ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:05<00:00,  8.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  üíæ Saved: ./images/42/42/output_green.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] Ï∂îÎ°† Ïã§Ìñâ (Î∞òÎ≥µ Ïã§Ìñâ Í∞ÄÎä•)\n","# ==========================================\n","\n","# 1. ÏõêÌïòÎäî ÏÉâÏÉÅÍ≥º ÏÑ§Ï†ï ÏûÖÎ†•\n","TARGET_COLOR = 'blue'  # red, green, blue Îì± Î≥ÄÍ≤Ω\n","INFERENCE_PATH = \"./images/42/42\"\n","\n","# 2. Ïã§Ìñâ Ìï®Ïàò Ìò∏Ï∂ú\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"id":"rgo-ZINAXdxX","executionInfo":{"status":"ok","timestamp":1765207118699,"user_tz":-540,"elapsed":14562,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6d491e1-47f2-4e76-b12a-dcacc5bf32ba"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Ïñ¥ÎåëÌÑ∞ ÍµêÏ≤¥ Ï§ë... (green -> blue)\n","‚úÖ blue Ïñ¥ÎåëÌÑ∞ Ïû•Ï∞© ÏôÑÎ£å!\n","\n","üìÇ Ï≤òÎ¶¨ ÏãúÏûë: ./images/42/42 (Color: blue)\n","‚ñ∂ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:05<00:00,  8.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  üíæ Saved: ./images/42/42/output_blue.jpg\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# [Cell 2] Ï∂îÎ°† Ïã§Ìñâ (Î∞òÎ≥µ Ïã§Ìñâ Í∞ÄÎä•)\n","# ==========================================\n","\n","# 1. ÏõêÌïòÎäî ÏÉâÏÉÅÍ≥º ÏÑ§Ï†ï ÏûÖÎ†•\n","TARGET_COLOR = 'purple'  # red, green, blue Îì± Î≥ÄÍ≤Ω\n","INFERENCE_PATH = \"./images/42/42\"\n","\n","# 2. Ïã§Ìñâ Ìï®Ïàò Ìò∏Ï∂ú\n","run_process(\n","    inference_root=INFERENCE_PATH,\n","    color=TARGET_COLOR,\n","    guidance_scale=9.0\n",")"],"metadata":{"id":"9rOnggCBXd7G","executionInfo":{"status":"ok","timestamp":1765207131514,"user_tz":-540,"elapsed":12807,"user":{"displayName":"ÏßÄÎä•Ï†ïÎ≥¥ SWÏïÑÏπ¥Îç∞ÎØ∏5Ï°∞","userId":"16997830524223997531"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ad8444d-b577-4f6a-90ef-15a10997be1b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Ïñ¥ÎåëÌÑ∞ ÍµêÏ≤¥ Ï§ë... (blue -> purple)\n","‚úÖ purple Ïñ¥ÎåëÌÑ∞ Ïû•Ï∞© ÏôÑÎ£å!\n","\n","üìÇ Ï≤òÎ¶¨ ÏãúÏûë: ./images/42/42 (Color: purple)\n","‚ñ∂ Processing: 42\n","Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n","Running DDIM Sampling with 50 timesteps\n"]},{"output_type":"stream","name":"stderr","text":["DDIM Sampler: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:05<00:00,  8.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  üíæ Saved: ./images/42/42/output_purple.jpg\n"]}]}]}